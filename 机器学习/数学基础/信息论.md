


# 信息论

[TOC]


## 熵
### 定义
* 用于度量事务的不确定性
* 不确定性越大，熵越大

### 公式
$$
H(X) = - \sum_{i=1}^{n} p_i log p_i
$$
说明
* $p_i$表示$X$取值为$i$的概率
* $log$表示以e为底的对数

联合熵
$$
H(X, Y) = - \sum_{x_i \in X} \sum_{y_i \in Y} p(x_i, y_i) log p(x_i, y_i)
$$

条件熵
$$
H(Y | X) = - \sum_{x_i \in X} \sum_{y_i \in Y} p(x_i, y_i) log p(y_i | x_i)
$$

### 关系
![](https://gitee.com/cc12703/figurebed/raw/master/img/20201229152536.png)

说明
* $I(X,Y)$称为 互信息、信息增益

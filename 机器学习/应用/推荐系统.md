
# 推荐系统

[TOC]


## 综述

### 预测
#### 定义
* 提前将可能的连接找出来呈现给用户

#### 类型
##### 评分预测
* 评分是一种用户的显示反馈

###### 问题
* 数据不容易收集
* 数据质量不能保证
* 数据不稳定

##### 行为预测
* 行为是一种用户的隐式反馈

###### 方式
* CRT预估：直接预测本身发送的概率
* 预测物品的相对排序

### 顽疾
* 冷启动：指新用户、不活跃用户、新物品、展示少的物品
* 探索与利用：大部分推荐感兴趣的，小部分试探新兴趣
* 安全：系统被攻击


### 阶段
#### 挖掘
* 内容：对用户、物品做深度挖掘，进行结构化分析
* 输出：物品画像、用户画像
* 运行：离线进行

#### 召回
* 内容：为一个用户从全量物品中筛选出一部分比较可靠的物品
* 输出：不同算法得到的推荐结果和推荐分数

#### 排序
* 内容：对召回的物品做统一的打分和排序


### 检测指标
#### 检测系统有多好
##### 深度类指标
* RMSE（均方根误差）：检测评分准确度
* AUC指标： 检测排序能力
* TopN指标

##### 广度类指标
###### 覆盖率
* 目的：反映推荐算法发掘长尾的能力
* 定义：最终的推荐列表中包含多大比例的物品

###### 失效率
* 目的：衡量推荐不出结果的情况



## 内容推荐

### 用户画像
#### 概念
* 给机器看的，不是给人看的
* 用户向量化后的结果
* 构建推荐系统中产生的一个副产品

#### 分类
* 稀疏向量：标签、注册资料、社交关系
* 稠密向量: Embedding向量、隐因子

#### 要点
* 每个维度的量化需要有机器完成
* 以目标为导向：推荐效果好坏

#### 来源
* 物品画像 + 用户行为


### 标签
#### 概念
* 通过挖掘文本数据来获取
* 是用户和物品共享的

#### 数据来源
##### 用户
* 注册资料
* 产生的内容：评论、日记
* 发生连接关系的文本：阅读过的内容

##### 物品
* 标题、描述
* 内容
* 基本属性


#### 挖掘
* 思想：使用NLP算法来分析文本信息

##### 方法
###### 关键词提取
* 作用：最基础的标签来源，提供基础数据
* 方法：TF-IDF,TextRank

###### 实体识别
* 作用：识别文本中有特定含义的实体
* 方法：词典法，HMM(隐马尔科夫模型)，CRF(条件随机场)

###### 文本分类
* 作用：将文本按照分类体系进行分类
* 方法：SVM

###### 文本聚类
* 作用：在无分类体系下，无监督地将文本划分多个类簇
* 方法
    * 基于距离: 传统的数据挖掘方法、计算量大、效果差
    * 基于主题模型: LDA模型

###### 嵌入向量
* 作用：通过学习得到一个对应的稠密向量
* 范围：词、文章


#### 选择
* 含义：将物品的结构化信息传递给用户

##### 思想
* 将用户对物品的行为（是否消费）作为一个分类问题
* 用户用实际行动标记了若干数据
* 将标签选择转变成特征选择

##### 方法
* 卡方检验
* 信息增益

##### 流程
1. 把物品结构化内容看成文档
1. 把用户行为看成类别
1. 每个用户见过的物品看成一个文本集合
1. 对该文本集合使用特性选择算法来获取用户关心的东西



### 推荐
* 最重要的是内容挖掘和分析，推荐算法反而不是很重要

#### 分析成果
##### 结构化的内容库
* 作用：结合用户行为去学习用户画像 

##### 内容分析模型
* 作用：实时的推荐新进入的物品

###### 流程
* 使用模型实时分析物品，提取结构化内容
* 和用户画像做匹配计算

###### 模型
* 分类模型
* 主题模型
* 实体识别模型
* Embedding模型


#### 算法
##### 相似性
* 做法：计算用户向量和物品向量的余弦相似度，按相似度排序

##### 机器学习
###### 做法
1. 收集行为日志，转换成训练样本
2. 训练预估模型（逻辑回归、梯度提升树）
3. 计算用户行为的发生概率，按概率排序

###### 样本
* 特征：用户画像、物品画像、场景信息（时间、地理位置、设备）
* 标准信息：用户行为（有反馈、无反馈）




## 近邻推荐

### 协同过滤算法
#### 关键
* 仅仅基于用户行为的算法
* 基于用户和物品的关系矩阵

#### 分类
* 基于记忆的：记录每个用户消费过什么的物品，给用户推荐相似的物品
* 基于模型的：从关系矩阵中学习一个可以泛化的模型

#### 核心问题
* 向量的构造
* 相似度计算方法的选择


### 相似度
* 假设：如果两个物体很相似，即距离很近

#### 计算方法
* 欧氏距离
* 余弦相似度
* 皮尔逊相关系数
* Jaccard相似度



### 基于用户的协同过滤（UserCF）
#### 原理
* 根据历史消费行为找到一群相似用户
* 推荐相似用户消费的，而该用户未见过的物品

#### 特点
* 推荐结果更加社会化，反映了群体中物品的热门程度
* 需要维护一个用户相似度矩阵


#### 流程
1. 构建关系矩阵（行是用户，列是物品）
2. 使用用户向量，计算用户间的相似度
    * 相似度公式: 余弦相似度, Jaccard公式
3. 构建推荐列表

##### 构建推荐列表方法
1. 汇总相似用户的喜欢物品
1. 去除用户已经消费过的物品
1. 对每个物品进行打分
    * 思想：选择k个相似用户打分的加权平均值
    * 公式：分数 = （相似用户打分　×　用户相似度）总和／　用户相似度之和　
1. 将物品按打分排序输出


#### 问题
* 用户数量多，计算相似用户会成为瓶颈
* 用户爱好变化快，不是静态的
* 用户间的共同消费行为比较少


#### 改进方案
##### UserCF-IIF
###### 要点
* 计算相似度时增加惩罚项，减少热门物品的影响

###### 公式
$$
w_{uv} =  \frac{ \sum_{i\in C} \frac{1}{log(1 + |N(i)|)} }
               { \sqrt{ |N(u)| \times |N(v)|}  } 
$$
$$
C = N(u) \cap N(v)
$$

* 该公式适用于隐式反馈
* N(i) 为用户i的正反馈物品集合


##### UserCF-Time
###### 要点
* 引入时间信息
* 计算相似度时增加时间衰减因子，时间间隔越长相似度越小
* 预测打分时增加时间衰减因子

###### 公式
$$
w_{uv} =  \frac{ \sum_{i\in C} \frac{1}{1 + \alpha |t_{ui} - t_{vi}|} }
               { \sqrt{ |N(u)| \times |N(v)|}  }
$$
$$
C = N(u) \cap N(v)
$$

* 该公式适用于隐式反馈
* N(i) 为用户i的正反馈物品集合
* t 为用户对物品产生行为的时间
* $\alpha$为超参数

$$
p(u,i) = \sum_{v\in S(u,K)} w_{uv} r_{ui} 
          \frac{1}{1 + \alpha (t_0 - t_{vi})}
$$

* S 包含了用户u兴趣最近的K个用户
* r 为行为数据，值为0或1
* $\alpha$为超参数



### 基于物品的协同过滤（ItemCF）
#### 原理
* 根据用户的历史行为找到相似的物品

#### 特点
* 推荐结果更加个性化，反映用户自己的兴趣
* 需要维护一个物品相似度矩阵

#### 流程
1. 构建关系矩阵（行是物品、列是用户，值：评分、行为）
2. 使用物品向量，计算物品的相似度
    * 目的：找到相似物品集合
    * 相似度公式: 余弦相似度
3. 构建推荐列表

##### 构建推荐列表方法
1. 汇总用户消费过物品的相似物品
1. 对每个物品进行打分
    * 思想：已消费物品打分的加权平均值
    * 公式：分数　＝　（已消费物品分数　×　物品相似度）总和 ／　物品相似度之和
1. 将物品按打分排序输出

#### 问题
* 相似度矩阵无法实时更新
* 没有考虑相似度的置信问题

#### 改进方案
##### ItemCF-IUF
###### 要点
* 计算相似度时增加用户活跃度参数，减少活跃用户的影响

###### 公式
$$
w_{uv} =  \frac{ \sum_{i\in C} \frac{1}{log(1 + |N(i)|)} }
               { \sqrt{ |N(u)| \times |N(v)|}  } 
$$
$$
C = N(u) \cap N(v)
$$

* 该公式适用于隐式反馈
* N(i)为物品i的用户集合


##### 物品相似度归一化
* 公式：w-ij / max(w-ij)

##### ItemCF-Time
* 引入时间信息
* 类似于UserCF-Time



### Slope One算法
#### 特点
* 专门针对评分矩阵
* 计算的是物品之间的距离，相似度的反面
* 一个物品至少要有两个用户评分

#### 方法
1. 计算物品的距离矩阵
    * 使用共同用户的评分来计算平均距离
    * 值：平均距离（共同的评分用户数）
1. 根据距离矩阵预测评分
    * 方式：利用用户已经评分的物品和物品之间的评分距离
    * 使用共同评分用户数进行加权平均


### 基于矩阵分析的协同过滤 
#### 概述
* 将关系矩阵分解成两个小矩阵的乘积
* 一个矩阵是用户矩阵，每个行向量是一个用户的隐因子向量
* 一个矩阵是物品矩阵，每个行向量是一个物品的隐因子向量
* 计算推荐得分时，直接用用户向量和物品向量做点积

#### 分类
##### Point-Wise
* 含义：先对单个用户对单个物品的偏好进行预测，后将结果进行排序
* 缺点：只能收集到正样本，将缺失值认为是负样本

##### Pair-Wise
* 含义：直接预测两两物品之间的相对顺序    

#### One-Class
* 作用：将评分预测问题转换成行为预测问题
* 思想：把预测行为看成二分类问题（用户是否会做某件事）
* 缺点：数据只有明确的正类别（用户做了某件事），负类别是假设的


#### 算法
* SVD（奇异值分解）
* 交替最小二乘（ALS）
* 加权交替最小二乘（Weighted-ALS）
* 贝叶斯个性化排序（BPR）

#### Weighted-ALS
##### 总体
* ALS的增加隐式反馈的改进算法
* 使用One-Class进行转换
* 加权：行为发生的次数是对行为的置信度的反映

##### 改进点
* 无隐式反馈，认为评分是0
* 有隐式反馈，认为评分是1，次数作为评分的置信度
* 使用负样本采样处理缺失值
    * 方法：按照物品的热门程度采样


#### BPR
##### 构造样本
* 四元组：用户，物品1，物品2，物品的相对顺序
* 相对顺序
    * 正样本：物品1消费过，物品2未消费过。值为1
    * 负样本：物品1未消费过，物品2消费过，值为0

##### 目标函数
* 含义：满足物品相对排序最佳
* 做法：计算正样本和负样本的分数之差

##### 参数训练
* 实现过程和SVD类似，只是参数更新公式不同

##### 注意点
* 不适合预测评分、只适合预测行为
* 所有样本都要采用曝光给用户的物品
    * 正样本：用户看见后有隐式反馈的物品
    * 负样本：用户看见后无隐式反馈的物品


#### 推荐计算

* 目的：降低复杂度

##### 使用专门的数据结构存储物品隐因子向量
* 开源实现: Faiss, KGraph, nmslib


##### 对物品隐因子向量进行聚类
###### 步骤
1. 将大量物品聚合成少量类
1. 计算用户和每个类中心的推荐分
1. 排序获取要推荐的类
1. 从类中挑选部分物品作为推荐结果



## 探索和利用

### 定义
* 探索：不断探索用户新的兴趣
* 利用：要用好用户比较确定的兴趣

### MAB问题（多臂老虎机问题）
* 定义：对于一排老虎机，如何才能最大化收益
* 推荐系统中的MAB问题: 冷启动, 探索和利用


### Bandit算法
#### 概念
##### 臂
* 每次选择的候选项  
* 每次推荐要选择的候选池（物品、策略、类型）

##### 回报
* 选择一个臂后得到的奖励 
* 用户是否喜欢推荐结果

##### 环境
* 系统无法控制的因素 
* 面对的用户

#### 思想
* 看选择会带来多少遗憾，遗憾越少越好
* 选择机会会倾向于那些确定好的和还不确定的选项

#### 汤普森采样算法
##### 思想
* 每个臂都对应一个概率分布（Beta分布）
* 选择时，让每个臂产生一个随机数。选择值最大的那个臂

##### 过程
1. 取出每个臂对应的参数，a 和 b
2. 使用a，b作为参数，用Beta分布生成一个随机数
3. 按随机数排序，选择最大值对应的臂
4. 观察用户反馈，用户点击将对应臂的a加一，否则将b加一


#### UCB算法（置信区间上界）
##### 思想
* 使用一个评分公式为每个臂评分
* 选择评分最高的臂

##### 公式
* 定义：收益（效果） + bonus（不确定性）
* bonus：
    * 置信区间上边界
    * 臂的选择次数很少时，bonus值就会较大，排序输出时有优势


#### Epsilon贪婪算法
##### 过程
1. 选择一个0,1之间较小的数据，叫做e
1. 每次以e的概率从所有臂中随机选择一个
1. 以1-e的概率去选择平均收益最大的臂

### Bandit算法-应用
#### 冷启动
* 用Topic来表示每个用户的兴趣
* 如果用户对某个Topic感兴趣，就表示我们得了收益
* 针对新用户，用汤普森采样算法为每个Topic采样一个随机数。排序后输出TopK的物品
* 根据用户反馈，更新对应Topic的值


## 模型融合

### 作用
* 对不同算法得出的推荐结果进行统一打分和排序，再推荐给用户

### 辑度组合
* 定义：一种融合方案，组合了逻辑回归（LR）和梯度提升决策树（GBDT）
* 原理：预估CTR（用户点击物品的概率），以该预估值作为标准进行统一排序


#### 子算法
##### LR
* 预估CTR，对样本做0-1二分类，0表示曝光后不被点击
* 特征中需要包含二阶的特征组合
* 使用学习算法学习权重

##### GBDT
* 对原始特征进行各种有效的组合
* 一个集成模型，由多棵决策树构成

#### 结合
1. 将样本的特征拼接在一起（用户、物品、场景）
2. 经过N棵树各自预测，得到N个预测结果
3. 将N个预测结果做One-Hot编码，拼接成一个向量
4. 使用逻辑回归处理这个向量，产生最终的预测结果


### 因子分解机（FM模型）
#### 特点
* 解决高维稀疏数据中的特征组合问题
* 适用于分类特征
* 适用于稀疏数据

#### 优点
* 在稀疏数据中进行合理的参数估计
* 时间复杂度是线性的
* 一个通用模型，用于任何特征为实值的情况


#### 起源
* 逻辑回归需要加入各种特征组合（最简单的就是特征的二阶笛卡尔乘积）
* 逻辑组合的问题
    * 导致特征维度灾难
    * 组合后的特征大部分可能是无效的
    * 组合后的特征非常稀疏


#### 思想
* 通过学习得到每一个特征的隐因子向量
* 在需要组合时，两个特征的隐因子向量做点积得到权重

#### 训练
* 预测值公式：线性回归 + 特征组合（特征隐因子向量的点积）
* 损失函数：Logistic Loss 函数
* 方法：随机梯度下降

#### 扩展模型（FFM模型）
* 特点：加入特征和特征类型之间的关系

### Wide&Deep模型
#### Wide模型
* 使用逻辑回归模型

#### Deep模型
* 是一个前馈神经网络
* 由输入层、隐藏层、输出层构成
* 隐藏层激活函数常使用ReLU
* 输出层函数：二分类使用Sigmoid，多分类使用Softmax

#### 融合
* 逻辑回归作为最终输出单元
* Deep模型的最后一个隐藏层作为特征输入逻辑回归
* 宽模型的原始特征一起输入逻辑回归
* 使用集成学习，将Deep模型、Wide模型放在一起进行参数学习

#### 数据生成
* 每条曝光日志生成一条样本，标签为0、1
* 字符串形式的特征映射为ID，需要过滤掉样本少的特征
* 连续值特征做归一化，使用累积分布函数划分N个分位

#### 模型训练
##### 步骤
1. 每个特征映射成一个32维的嵌入向量
1. 将所有的特征的嵌入向量合并成一个大向量
1. 大向量进入三层的神经网络
1. 最终输出后计算Logistic Loss，再反向传播误差

##### 特点
1. 新样本训练时，会使用上一次的模型来初始化模型参数

#### 模型应用
* 每次请求时，获取召回模块的候选列表，以及用户特征
* 对候选列表的物品进行评分
* 根据评分进行排序


## 深度学习




## 其他算法

### 排行榜
#### 用途
* 解决新用户的冷启动
* 发现老用户的兴趣
* 相当于一个降级的推荐系统，作为兜底方案

#### 实现
##### 基础方法
* 统计某种指标
* 按照大小排序

##### 考虑时间
* 思想：排行榜中的物品都有一定的温度，会随时间的推移而不断下降
* 公式： 牛顿冷却算法

##### 考虑反对票
* 思想
    * 同样多的总票数，支持赞成票多的
    * 同样多的赞成票数，支持最有价值的

##### 考虑好评率
* 思想：使用好评率作为核心
* 公式 威尔逊区间

#### 防止刷榜
* 投票准入：有一定价值的用户才能投票
* 一人一票制
* 按权重计算：按用户影响力计算用户权重


### 采样算法
* 问题：召回计算时应该使用多少标签
* 方案：每次召回时，使用一个加权采样算法获取一部分标签来使用

#### 优点
* 大大减少召回计算的复杂度
* 可以保留更多的用户标签
* 每次召回时标签还能有所变化

#### 算法
##### 有限数据集
* 使用：加权采样公式
* 公式1：S = R ^ (1 / w)
    * R 为0-1之间的随机数
    * w 为标签的权重
* 公式2：指数分布的随机数

##### 无限数据集
* 方法：蓄水池采样    
* 前提：数据集合，一共n个样本，从中采样取出k个样本
* 做法
    * 先取出前k个样本保留，需要随时准备输出的
    * 从第k+1个样本开始，每个都以k/n的概率去替换保留集中的样本


### 重复检测

#### 生产端内容重复检测
##### Simhash算法
###### 思想
* 为每个内容生成一个整数表示的指纹
* 使用该指纹进行检测

###### 步骤
1. 进行分词
2. 将词进行哈希，获得一个0/1向量
3. 将0换成-1，获得一个1/-1向量
4. 向量乘以权重，获得一个加权向量
5. 所有词的加权向量相加，获得一个最终的求和向量
6. 正数换成1，负数换成0，获取一个0/1向量（二进制）
7. 该向量就是内容的指纹

#### 消费端避免重复推荐
* 做法：使用布隆过滤器去检测内容ID
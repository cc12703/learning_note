
# 线性算法

[TOC]


## 线性回归

### 概述
* 思想：用一条直线去拟合一组观测数据
* 目标函数：最小化均方误差
* 求解算法：梯度下降法

### 模型
#### 映射函数
$$
f(x) = {\omega} \cdot x + b
$$

说明
* $\omega$ 和 x 都是向量

#### 目标函数 
$$
\min_{\omega} \frac{1}{2} (X\omega - Y)^T (X\omega - Y)
$$


### 求解
#### 更新方程
$$
\omega = \omega - \alpha X^T (X\omega - Y)
$$

说明
* $\alpha$ 为步长


### 正则化
#### 目的
* 防止模型过拟合，增加泛化能力

#### Lasso回归
* 定义：在目标函数上增加一个L1正则化的项
* 作用：使得一些特征的系数变小，甚至直接变成0

##### 目标函数
```math
\min_{\omega} \frac{1}{2} (X\omega - Y)^T(X\omega - Y)
             + \alpha {\left \| \omega \right \|}_1 
```

#### Ridge回归
* 定义：在目标函数上增加一个L2正则化项
* 作用：缩小回归系数，使得模型相对比较稳定

##### 目标函数
```math
\min_{\omega} \frac{1}{2} (X\omega - Y)^T(X\omega - Y) 
            +  \alpha {\left \| \omega \right \|}_2 
```


## 感知机

### 概述
* 二分类模型，要求数据是线性可分的
* 思想：使用分离超平面来将数据分成两个部分
* 目标函数：最小化所有误分类点到分离超平面的总距离
* 求解算法：随机梯度下降法

### 模型
#### 分离超平面
$$
\omega x + b = 0
$$
说明
* $x \in R^n$, n为特征空间大小
* $\omega$ 和 $x$ 都是向量

#### 映射函数
$$
f(x) = sign(\omega x + b)
$$
说明
* sign为符号函数，输出+1、-1

### 原始形式
#### 目标函数
$$
\min_{\omega,b}L(\omega, b) = - \sum_{ x_i \in M } y_i (\omega * x_i + b)
$$
说明
* M为所有误分类点的集合

#### 推导过程
1. 点到分离平面的距离  
$$ \frac{|\omega x_0 + b|}{||\omega||} $$
1. 误分类点到分离平面的距离
$$ - y_i (\omega \cdot x_i + b) > 0 $$
1. 所有误分类点到分离平面的距离
$$
-\frac{1}{||\omega||} \sum_{ x_i \in M } y_i (\omega * x_i + b)
$$
1. 分子和分母都含有$\omega$，有固定的倍数关系，所以可以固定分母为1
    * 不会影响求极值
    * 不会影响终止条件
$$
-\sum_{ x_i \in M } y_i (\omega * x_i + b)
$$

#### 求解
##### 更新公式
$ \omega = \omega + \eta y_i x_i $
$ b = b + \eta y_i $
说明
* $\eta$ 为步长

##### 误分类判断公式
$$ y_i (\omega \cdot x_i + b) \le 0 $$

##### 步骤
1. 设置初始值：$\eta$, $\omega$, $b$
1. 从训练集中选择一个样本
1. 使用误分类判断公式，判断是否是误分类样本
1. 若是误分类样本，使用更新公式更新 $\omega$, $b$
1. 进入步骤2，直到训练集中没有误分类样本


### 对偶形式
#### 目的
* 优化算法的执行速度（在特征多，样本少时起作用）

#### 原理
* 将$\omega$, $b$ 表示为 $x$，$y$的线性组合
* 将每轮迭代复杂度，从**特征空间维度**转移到了**训练集大小**上

##### 公式
$$
\omega = \sum_{i=1}^{N} \alpha_i y_i x_i
$$
$$
b = \sum_{i=1}^{N} \alpha_i y_i
$$
说明
* $\alpha_i = n_i \eta$, n为样本被误分类的次数
* N为样本数据个数

#### 求解
##### 更新公式
$ \alpha_i = \alpha_i + \eta $
$ b = b + \eta y_i $
说明
* $\eta$ 为步

##### 误分类判断公式
$$ 
y_i (\sum_{j=1}^{N} (\alpha_j y_j x_j) \cdot x_i + b) \le 0 
$$
说明
* 可以预先计算出样本数据的內积，称为Gram矩阵
$$ G = [x_i \cdot x_j]_{N \times N} $$

 


## 逻辑回归
### 概述
* 二分类模型, 属于对数线性模型
* 目标函数：通过二项分布，使用极大似然法推导出
* 求解算法：梯度下降算法

### 模型
#### 函数
$$
f(x) = \frac{1}{ 1 + e^{ - ({\omega}^Tx + b)} }
$$

说明
* $\omega$ 和 x 都是向量
* 对线性回归的结果，使用sigmoid函数进行转换
* 输出值为 0 -- 1 之间
* 判断分类结果时，可以使用0.5作为阀值

#### 目标函数
$$
\min_{\omega} - Y^T log(h_{\omega}(X)) - (E - Y)^T log(E - h_{\omega}(X))
$$
说明
* E为全1向量
* $ h_{\omega}(x) = \frac{1}{1 + e^{-x \omega}}$

##### 推导过程
1. 根据二元分布，获的概率函数
$$
P(y|x, \omega) = h_{\omega}(x)^y (1 - h_{\omega}(x))^{1 - y}
$$
1. 获得似然函数
$$
L(\omega) = \prod_{i=1}^{m} (h_{\omega}(x_i))^{y_i} 
                (1 - h_{\omega}(x_i))^{1-y_i} 
$$
1. 对似然函数进行对数化，取反即可得到目标函数


### 求解（梯度下降法）
#### 更新方程
$$
\omega = \omega - \alpha X^T (h_{\omega}(X) - Y)
$$
说明
* $\alpha$ 为步长
* $X, Y$都是向量



## 最大熵模型

### 概述
* 属于对数线性模型
* 属于分类算法
* 求解方法：凸优化技术

### 模型
* 一个条件概率分布$P(Y|X)$, $X$为特征，$Y$为输出
* 目标是求条件熵最大时对应的$P(y|x)$


#### 条件熵
$$
H(P) = - \sum_{x, y} \overline{P}(x) P(y|x) log P(y|x)
$$

#### 约束条件
$$ E_{\overline{P}}(f) = E_P(f) $$

特征函数
$$
f(x, y) = 
    \begin{cases}
    1 & \text{ x与y满足某个关系 } \\
    0 & \text{ 否则 }
    \end{cases}
$$

期望值
$$
E_{\overline{P}}(f) = \sum_{x,y} \overline{P}(x,y) f(x,y)
$$

$$
E_P(f) = \sum_{x,y} \overline{P}(x) P(y|x) f(x,y)
$$





## 支持向量机（SVM）

### 概述
* 二分类模型
* 使用分离超平面来分隔样本数据
* 目标函数：最大化样本到超平面的间隔
* 求解算法：凸二次规划优化法

#### 支持向量
* 样本中与分离超平面距离最近的样本实例
* 在分离超平面中起着决定性作用

#### 分类
* 线性可分SVM：使用硬间隔最大化，样本需要线性可分
* 线性SVM：使用软间隔最大化，样本要近似线性可分
* 非线性SVM：使用核技巧、软间隔最大化，样本线性不可分

### 间隔
* 一个样本点距离分离超平面的远近
* 该间隔可以近似表示分类预测的置信度

#### 函数间隔
$$
\hat{\gamma_{i}} = y_i (w \cdot x_i + b) 
$$

#### 几何间隔
$$
\gamma_{i} = y_i (\frac{\omega}{\left\| \omega \right\|} \cdot x_i 
    + \frac{b}{\left\| \omega \right\|} ) 
$$

### 线性可分SVM算法
#### 原始优化问题
$$
\min_{\omega, b} \frac{1}{2} {\left \| \omega  \right \|}^2
$$
s.t. 
$$ 
y_i(\omega \cdot x_i + b) - 1 \ge 0 , i = 1,2,3...,N 
$$

##### 求解步骤
1. 根据优化问题，求解出最优解 $ \omega^* , b^* $
1. 生成分类决策函数 $ f(x) = sign( \omega^* \cdot x + b^*) $

#### 对偶优化问题
$$
\min_{\alpha} \frac{1}{2} 
          \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j (x_i \cdot y_j)
           - \sum_{i=1}^{N} \alpha_i
$$
s.t.
$$ 
\sum_{i=1}^{N} \alpha_i y_i = 0
$$
$$
\alpha_i \ge 0, i=1,2,...,N
$$

##### 求解步骤
1. 根据优化问题，求解出最优解：$ \alpha^* = (\alpha_{1}^* + \alpha_{12}^*+ ... + \alpha_{N}^*)^T $
1. 计算出 $ \omega^* , b^* $
1. 生成分类决策函数 $ f(x) = sign( \omega^* \cdot x + b^*) $


### 线性SVM算法
#### 原始优化问题
$$
\min_{\omega, b, \xi} \frac{1}{2} {\left \| \omega  \right \|}^2
                       + C \sum_{i=1}^{N}\xi_i
$$
s.t.
$$ 
y_i(\omega \cdot x_i + b) - 1 \ge 1 - \xi_i , i = 1,2,3,...,N
$$
$$
\xi_i \ge 0 , i = 1,2,3,...,N
$$

说明
* C为调和系数， $\xi_i$为松弛变量

#### 对偶优化问题
$$
\min_{\alpha} \frac{1}{2} 
          \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j (x_i \cdot y_j)
           - \sum_{i=1}^{N} \alpha_i
$$
s.t.
$$ 
\sum_{i=1}^{N} \alpha_i y_i = 0
$$
$$
0 \le \alpha_i \le C, i=1,2,...,N
$$


### 非线性SVM算法
#### 优化问题
$$
\min_{\alpha} \frac{1}{2} 
          \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j K(x_i \cdot y_j)
           - \sum_{i=1}^{N} \alpha_i
$$
s.t.
$$ 
\sum_{i=1}^{N} \alpha_i y_i = 0
$$
$$
0 \le \alpha_i \le C, i=1,2,...,N
$$

说明
* K 

##### 求解步骤
1. 根据优化问题，求解出最优解：$ \alpha^* = (\alpha_{1}^* + \alpha_{12}^*+ ... + \alpha_{N}^*)^T $
1. 计算出 $ b^* $
1. 生成分类决策函数 $ f(x) = sign( \alpha_{i}^* y_i K(x, x_i) + b^*) $


### 核技巧
#### 思路
* 进行一个非线性变换，将非线性问题变换为线性问题
* 从原始空间映射到更高维的特征空间，使样本在特征空间线性可分

#### 核函数
##### 优点
* 在学习、预测时，只使用核函数
* 不显式定义映射函数

##### 定义
$$  K(x, z) = \Phi(x) \cdot \Phi(z) $$

说明
* $\Phi$为映射函数
* x为原始空间的样本
* z为特征空间的样本

##### 常用核
* 多项式核
* 高斯核
* 字符串核

#### SVM应用
* 将对偶问题中目标函数的內积用核函数代替


### SMO算法
* 全称：序列最小最优化算法 
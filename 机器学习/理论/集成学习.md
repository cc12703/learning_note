
# 集成学习

[TOC]

## 概述

### 具体算法
* 随机森林
* Adaboost

### 特点
* 既可以用于分类问题、也可以用于回归问题
* 基础模型一般都是树模型

### 策略
#### bagging（装袋）
##### 思路
* 将多个基础模型并行集成

##### 步骤
1. 对数据集进行随机采样
2. 分别训练多个基础模型
3. 将多个预测结果求平均值

##### 例子
* 随机森林

#### boosting（提升）
##### 思路
* 将基础模型一个一个串行加入

##### 步骤
1. 生成一个基础模型
2. 不断加入新模型，前提加入后会使整体效果变好
3. 将多个模型的结果加在一起

##### 例子
* Adaboost
* 随机梯度提升（GBM）

#### stacking
##### 思路
* 集成不同算法的模型

##### 步骤
1. 选择m个不同分类器对数据进行建模
2. 将第一阶段的结果当作数据特征传入总分类器
3. 由总分类器获得最终结果

##### 细节
* 将数据分成多份，每个分类器使用其中的一份



## 随机森林

### 思想
* 生成多个不同的树模型，将结果平均

### 概念
* 森林：多个决策树
* 随机（二重随机性）
    * 随机数据采样，每颗树的训练数据来自数据集的一部分
    * 随机特征选择，每颗树选择所有特征的一部分

### 结果集成
* 分类任务求众数
* 回归任务求平均值

### 特点
* 并行形式：多个树模型之间是无关的，使用相同的参数
* 可以进行可视化展示
* 总是先使用最好的特征




## Adaboost

### 思路
* 给样本赋予不同的权重
* 做对的样本，给较低权重
* 做错的样本，给较高权重，让模型更重视
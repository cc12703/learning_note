
# 集成学习

[TOC]

## 概述

### 思想
* 用训练数据，训练出若干个个体学习器
* 通过一定的结合策略，最终形成一个强学习器

### 主要问题
#### 如何选择个体学习器
* 同质的：所有的学习器都是同一种类型
    * 应用最广泛
    * 最多的类型是 CART决策树、神经网络
* 异质的：所有的学习器有多种类型

#### 如何整合个体学习器（同质）
* 学习器之间存在强依赖关系，需要串行生成
    * 算法：boosting
* 学习器之间不存在强依赖关系，可以并行生成
    * 算法：bagging、随机森林


### 方案
#### boosting系列
##### 原理
![](https://gitee.com/cc12703/figurebed/raw/master/img/20210106221036.png)

##### 流程
1. 从训练集用初始权重训练出一个弱学习器
1. 根据弱学习器的学习误差率来更新训练样本的权重（误差率高的样本，权重变高）
1. 用调整后权重训练出下一个弱学习器，直到指定个数
1. 使用结合策略进行整合，得到最终的学习器

##### 特点
* 做对的样本，给较低权重
* 做错的样本，给较高权重，让模型更重视 

##### 问题
* 如何计算**学习误差率**
* 如何得到**权重系数**
* 如何更新样本权重
* 使用那种结合策略

##### 算法
* Adaboost
* 提升树系列


#### bagging系列
##### 原理
![](https://gitee.com/cc12703/figurebed/raw/master/img/20210106222527.png)

##### 流程
1. 对训练集进行随机采样，训练出弱学习器
1. 使用结合策略进行整合，得到最终的学习器

##### 采样
* 自助采样法：每次采样后，样本都会放回

##### 算法
* 随机森林



### 结合策略
#### 平均法
* 对若个弱学习器的输出进行平均，得到最终结果
* 通常用于数值类型的回归问题

##### 类型
* 算术平均
* 加权平均：每个弱学习器有一个权重


#### 投票法
* 通常用于分类问题

##### 类型
* 相对多数投票：数量最多的类别为最终类别
* 绝对多数投票：数量最多且超过半数的类别为最终类别


#### 学习法
* 结合时使用一个独立的学习器重新学习一次

##### 流程
* 训练时
    1. 训练弱学习器
    1. 使用弱学习器的学习结果作为输入，训练集的输出为输出，训练结合学习器
* 预测时
    1. 用弱学习器预测一次
    1. 将弱学习器的预测结果，用结合学习器再预测一次






## Adaboost

### 概述
#### 特点
* 即可用于分类，也可用于回归

#### 优点
* 分类精度高
* 不容易发生过拟合
* 作为二分类器时，构造简单，结果可解释

#### 缺点
* 对异常样本敏感

#### 弱分类器类型
* 使用最广泛的是决策树和神经网络
* 对于决策树，分类用CART分类树，回归用CART回归树


### 思路（二分类）
#### 样本权重
$$ D(k) = (\omega_{k1}, \omega_{k2}, ..., \omega_{km}) $$
$$ \omega_{1i} = \frac{1}{m}; i = 1,2,3,...,m $$
说明
* $k$表示第k个弱学习器
* $m$为训练样本总数

#### 学习误差率
$$
e_k = \sum_{i=1}^m \omega_{ki} I(G_k(x_i) \ne y_i)
$$
说明
* $y$ 输出为 -1， 1
* $G_k(x)$ 为弱分类器
* $I$为指示函数，即$G_k(x_i) \ne y_i$时为1，否则为0

#### 权重系数
$$
\alpha_k = \frac{1}{2} log \frac{1 - e_k}{e_k}
$$


#### 权重更新
$$
\omega_{k+1, i} = \frac{\omega_{k,i}}{Z_k} 
                   exp( - \alpha_k y_i G_k(x_i)) 
$$

$Z_k$是规范化因子
$$
Z_k = \sum_{i=1}^m \omega_{k,i} exp( - \alpha_k y_i G_k(x_i))
$$

#### 结合策略
* 使用加权表决法

$$
f(x) = sign(\sum_{k=1}^K \omega_k G_k(x))
$$


### 流程（二分类）
1. 初始化样本集权重
1. 循环处理 k = 1,2,3,...,K (K为迭代次数)
    1. 使用带权重的样本，训练出弱分类器
    1. 计算弱分类器的分类误差率
    1. 计算弱分类器的权重系数
    1. 更新样本集的权重
1. 构建最终的强分类器


### 推导（二分类）
#### 原理
* 将Adaboost简化为一个加法模型（分类器串行）
* 学习算法为前向算法
* 损失函数为指数函数

##### 模型公式
$$ f(x) = \sum_{k=1}^{K} \alpha_k G_k(x) $$
说明
* $G_k(x)$为每一轮的分类器
* $\alpha_k$为分类器的权重

##### 损失函数
$$ L(y, f(x)) = exp[ -y f(x) ]$$

#### 推导后的损失函数
$$
(\alpha_k, G_k(x)) = \underbrace{\arg \min}_{\alpha, G} 
                    \sum_{i=1}^m \bar{\omega}_{ki} exp[- y_i \alpha G(x)]
$$
$$ \bar{\omega}_{ki} = exp[ - y_i f_{m-1}(x_i)]$$


##### 求$G_k(x)$
* $\bar{\omega}_{ki}$无$\alpha$和$G$都无关，最小化时可以看成常数
* $\alpha$也可以认为是一个常数
* 将损失函数变形可得
$$ 
    \sum_{i=1}^m \bar{\omega}_{ki} exp[- y_i \alpha G(x_i)] = 
        (e^{\alpha} - e^{-\alpha}) 
        \sum_{i=1}^m \bar{\omega}_{ki} I(y_i \ne G_k(x_i))
        + e^{-\alpha} \sum_{i=1}^{m} \bar{\omega}_{ki}
$$

最终得到
$$
G_k(x) = \underbrace{\arg \min}_{G} 
            \sum_{i=1}^m \bar{\omega}_{ki} I(y_i \ne G_k(x_i))
$$

##### 求$\alpha_k$
1. 将$G_k(x)$带入损失函数
1. 对$\alpha$求导，使其等于0



## GBDT 梯度提升树

### 概述
#### 特点
* 即可用于分类，也可用于回归
* 算法也是迭代的，也使用了前向分布算法
* 弱学习器只能用CART决策树

#### 优点
* 处理各种类型的数据：连续值、离散值
* 性能比较高

#### 缺点
* 比较难并行进行训练


### 思路
* 每轮迭代的目标是找到一个CART的弱学习器，让该轮的损失函数最小
* 用损失函数的**负梯度**来拟合该轮损失的近似值

#### 损失函数公式
$$  L(y, f_t(x)) = L(y, f_{t-1}(x)) + h_t(x) $$
说明
* $t$为训练轮次
* $h_t(x)$为该轮找到的弱学习器

#### 负梯度公式
$$
 r_{ti} = - \left[ \frac{\partial L(y_i, f(x_i))}{\partial f(x_i)} \right]_{f(x) = f_{t-1}(x)}
$$
说明
* $r_{ti}$值第$t$轮的第$i$个样本的损失函数的负梯度


### 流程-回归
1. 初始化学习器
$$ f_0(x) = \underbrace{\arg \min}_c \sum_{i=1}^m L(y_i, c) $$
1. 进行$T$轮迭代
    1. 计算所有样本的负梯度$r_{ti}$
    1. 使用$(x_i, r_{ti})$拟合一个CART回归树
    1. 对于叶子节点，就是最佳拟合值
    $$
        c_{tj} = \underbrace{\arg \min}_c 
                    \sum_{x_i \in R_{tj}} 
                        L(y_i, f_{t-1}(x_i) + c) 
    $$
    1. 更新强学习器
    $$ f_t(x) = f_{t-1}(x) + \sum_{j=1}^J c_{tj} I(x \in R_{tj})  $$
1. 获取最终的学习器
    $$
        f_T(x) = f_0{x} + \sum_{t=1}^T \sum_{j=1}^J c_{tj} I(x \in R_{tj})
    $$
    
符号说明
* $t$为迭代轮次
* $m$为样本个数
* $J$为决策树的叶子节点个数
* $R_{tj}$为第$t$颗树，第$j$个叶子节点的区域


### 常用损失函数
#### 分类
* 对数损失
* 指数损失

#### 回归
* 均方差
* 绝对损失
* Hubers损失
* 分位数损失


### 正则化
#### 步长法
$$ f_t(x) = f_{t-1}(x) + \nu h_t(x) $$
说明
* $\nu$为步长，取值范围$0 \le \nu \le 1$

#### 子采样法
* 采样使用不放回抽样
* 采样比例，推荐0.5至0.8之间

#### 剪枝法
* 对CART树进行正则化剪枝






## 随机森林(RF)

### 概述
#### 特点
* 弱学习器使用CART决策树
* 使用二重随机性
    * 随机数据采样，每颗树的训练数据来自数据集的一部分
    * 随机特征选择，每颗树选择所有特征的一部分

#### 优点
* 训练可以高度并行化
* 在特征维度很高的时候，仍然可以进行训练
* 模型的泛化能力强
* 实现比较简单
* 对部分特征缺失不敏感

#### 缺点
* 在噪音较大的样本上，容易出现过拟合


### 流程
1. 进行$T$轮循环
    1. 对训练集进行随机采样，得到一个采样集$D_t$
    1. 使用采样集训练CART模型$G_t(x)$。对节点上所有特征随机选择一个部分用于训练
1. 整合结果
    * 分类：$T$个弱分类器投票最多的类别为最终类别
    * 回归：$T$个弱分类器的结果的算术平均值为最终输出


### 推广
#### extra trees
* 原理与RF交互一模一样

##### 区别
* 训练每颗决策树时使用原始训练集
* 随机选择一个特征来作为划分节点


#### TRTE（Totally Random Trees Embedding）
##### 功能
* 是一种非监督学习的数据转化方法：将低维数据映射到高维


#### IForest（Isolation Forest）
##### 功能
* 是一种异常点检测方法

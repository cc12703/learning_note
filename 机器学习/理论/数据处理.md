
# 数据处理

[TOC]

## 分类


### 按数据类型分
#### 定量数据
* 本质上是数值
* 衡量某样东西的数量

#### 定性数据
* 本质上是类别
* 描述某样东西的性质


### 按数据等级分
#### 定类数据
* 属于定性数据
* 例子：血型、人名、动物物种
* 操作：计数
* 图表：条形图、饼图

#### 定序数据
* 属于定性数据，本质依然是类别，但是可以自然排序
* 例子：餐厅评分、考试成绩（A，B，C）
* 操作：计算中位数、计算百分位数
* 图表：箱线图

#### 定距数据
* 属于定量数据，值之间的差异存在意义
* 例子：温度
* 操作：加法、减法、计算均值、计算标准差
* 图表：直方图、散点图

#### 定比数据
* 属于定量数据，拥有 绝对零点 的概念
* 例子：货币
* 操作：乘法、除法



## 数据可视化

### 箱线图
#### 示例
![](https://gitee.com/cc12703/figurebed/raw/master/img/20201210105558.png)

#### 构成
* 箱子中间有一条线，表示中位数
* 箱子的上下线，表示第一个四分位数和第三个四分位数
* 上下各一条横线，表示上下边缘
* 一些离群点，表示异常值

#### 作用
* 可视化数据的分散度和偏度

#### 要点
* 一半的数据位于箱体内
* 离群点指大于1.5倍四分位距的值



### 散点图
#### 示例
![](https://gitee.com/cc12703/figurebed/raw/master/img/20201210105930.png)

#### 构成
* 点指对象
* 坐标轴指对象的不同属性



### Q-Q图
#### 构成
* 数据分位数和正态分布分位数的对比参照图
* 如果数据符合正态分布，所有点都会落在直线上



### KDE分布图
#### 构成
* 理解为对直方图的加窗平滑

#### 作用
* 对比训练集、测试集中特征变量的分布情况



## 特征工程

### 定义
* 特征：对机器学习有意义的数据属性
* 目的：获取更好的数据，以便机器学习获取更好的效果

### 评估方法
* 应用特征工程前，得到模型的基准性能
* 应用特征工程
* 获取一个性能指标，并与基准进行对比
* 若性能增量大于某阀值，则认为该特征工程有用（以百分比计算）


### 步骤

* 数据探索
    * 进行一些基本的描述性统计、可视化操作，来理解数据的性质
    * 仔细观察数据集的每一列和大致特点
* 数据构建
    * 将类别数据、文本数据转换成数值
* 特征增强、优化
    * 处理缺失数据
    * 处理异常数据
    * 无量纲化
    * 生成组合特征
* 特征选择
* 特征降维


## 数据探索

### 双变量分析
#### 连续型与连续型
* 绘制散点图：反映变量之间的线性关系
* 计算相关性

#### 类别型与类别型
* 双向表
* 堆叠柱状图
* 卡方检验

#### 类别型与连续型
* 小提琴图



## 特征构建


### 虚拟编码（one-hot编码）
#### 概述
* 用于定类数据
* 用于常规数据量
* 防止常规数值编码给算法引入入不准确的信息
    * 算法会把分类数据作为连续的，有大小的，可计算的数据来处理

#### 方法
* 将值转换成多维向量，向量值为0 或 1
* 维数为类型数量

#### 缺点
##### 数据有冗余
* 使用的比特位要比实际需要的多一位
* 导致模型会有多个解释

##### 陷阱
* 含义：若变量值之间可以进行预测，则会存在重复类型
* 例子：性别的值是可以相互推断的，转换后 female = 0, female = 1

#### 改进
* 忽略一个类别，该类别通过一个全零向量来表示


### 特征散列化
#### 概述
* 用于定类数据
* 用于大数据量（例如：用户ID，IP地址）
* 当特征值比较多时，用来压缩原始的特征向量

#### 原理
* 使用散列函数转换原始特征值

#### 优点
* 保持特征空间
* 减小训练和评价时所使用的存储空间和处理时间

#### 缺点
* 生成的数据缺乏解释性


### 分箱计数
#### 概述
* 用于定类数据
* 用于大数据量（例如：用户ID，IP地址）
* 将一个分类变量转换为与其相关的统计量

#### 思想
* 不直接使用分类变量的值，而是使用目标变量取这个值的条件概率

#### 优点
* 将一个大型的、稀疏的、二值的分类变量转换为一个小型的、密集的、实数型的数值表示



### 标签编码器
#### 概述
* 用于定序数据
* 按照数据的顺序来确定编码后值的大小

#### 方法
* 创建一个列表，使用其索引值作为数值数据


### 二值化编码
#### 概述
* 用于定量数据
* 将连续数据转换成 0或1

#### 方法
* 大于阀值时输出为1
* 小于阀值时输出为0


### 分箱（区间量化）
#### 概述
* 用于定量数据
* 将连续型数值映射为离散型数值

#### 固定宽度分箱
* 宽度可以人工定制，自动分段
* 宽度可以是线性的，指数的（数值跨多个数量级时）

#### 自适应分箱
* 使用数据分布的分位数来实现


### 词袋法
#### 概述
* 用于处理文本数据
* 通过单词的出现频率描述文档，忽略单词的位置

#### 步骤
1. 分词：将文档变成词项，每个词项由一个整数ID
1. 计数：计算文档中词项出现的次数
1. 归一化：将词项按重要性按逆序排列



### TF-IDF向量化 
#### 概述
* 用于处理文本数据
* TF-IDF：词频-逆文档频率

#### 优点
* 使用IDF减少了常见词的权重




## 特征增强、优化

### 填充缺失值
* 对于定量数据，使用均值、中位数
* 对于定类数据，使用最常见类别(数量最多的)

### 转换异常值
#### 对数变换
* 作用：改变数据的分布形状
* 适用：向右倾斜的分布
* 缺点：不能用于含有零、负值的数据

#### 平方根(立方根)变换
* 作用：改变数据分布的形状
* 可以处理含有零、负值的数据



#### Box-Cox变换
##### 作用
* 将数据转换成满足正态分布
* 减少不可观测误差和预测变量的相关性

##### 公式
$$
y(\lambda) = 
\begin{cases}
   \frac{y^{\lambda}_i - 1}{\lambda} & \text{ if } \lambda \ne 0 \\
   \ln_{}{y_i}  & \text{ if } \lambda = 0
\end{cases}
$$

* $\lambda$ 是超参数


### 无量纲化
#### 概述
* 将不同规格的数据转换到同一规格
* 将不同分布的数据转换到某个特定分布

#### 作用
##### 加快求解速度
* 以梯度和矩阵为核心的算法
* 逻辑回归、支持向量机、神经网络

##### 提升模型精度
* 以距离为核心的算法
* K紧邻，K-Means聚类

##### 特例
* 决策树和树集成算法
* 不需要无量纲化，可以处理任意数据

#### 方法
##### 中心化处理
* 原理：通过减去一个固定值，让样本数据平移到某个位置

##### 缩放处理
* 原理：通过除以一个固定值，让样本数据固定在某个范围内

##### z分数标准化（数据标准化）
* z分数指标准分数
* 输出数据会服从标准正态分布 （均值为0，标准差为1）
* 处理步骤：数据按均值中心化处理，再按标准差缩放
* 在大多数算法中用于特征缩放

##### min-max标准化（数据归一化）
* 输出数据会被收敛到 0 -- 1 之间
* 处理步骤：数据按最小值中心化处理，再按极差（最大值 －　最小值）缩放
* 问题：标准差都非常小，对异常值非常敏感


### 特征组合
#### 多项式特征
* 做法：使用原有列的乘积来创建新列
* 输出：生成一个新的特征矩阵，包含原始特征的多项式组合
* 作用：捕获特征交互




## 特征选择

### 作用
* 去除冗余数据，降低数据的过拟合度
* 减少误导数据，提高算法精度
* 减少训练数据，降低训练时间

### 元指标
#### 定义
* 不直接与模型预测性能相关的指标

#### 包括
* 模型拟合、训练所需要的时间
* 拟合后模型预测的时间
* 需要持久化的数据大小

### 分类
* 基于统计：依赖于统计测试
* 基于模型：依赖于一个预处理步骤，需要训练一个辅助的机器学习模型
* 单变量：每次对一个特征进行测试，衡量特征和响应变量之间的关系


### 方法

#### 皮尔逊相关系数
##### 特点
* 基于统计的，单变量方法
* 测试特性之间的线性关系

##### 值
* 范围：-1 -- 1
* 0 表示无线性关系
* 1，-1 表示很强的线性关系

##### 步骤
1. 计算特征和响应变量的相关性
2. 选择相关系统大于阀值的特征


#### 假设检验
##### 特点
* 基于统计的，单变量方法

##### 步骤
1. 定义假设：特征与响应变量没有关系（零假设）
2. 使用p值对每个特征进行检验
3. 保留p值低的特征

##### p值
* 范围：0 - 1 的小数
* 值越低，拒绝零假设的概率越大
* 含义：在假设检验下，给定数据偶然出现的概率

##### 统计指标
* 卡方统计
* ANOVA统计


#### 基于树的模型
##### 原理
* 使用构建树节点时所使用的重要性指标来选择特征

##### 模型
* 决策树
* 随机森林

#### 基于线性模型
##### 原理
* 使用模型给特征设置的参数来选择特征
* 利用了线性模型的正则化功能

##### 模型
* 线性回归
* 逻辑回归
* 支持向量机（SVM）

### 方法选用

#### 特征是分类的
* 卡方检验
* 树模型

#### 特征是定量的
* 线性模型
* 相关性检验

#### 二元分类问题
* SVC模型（支持向量分类器）



## 特征降维

### 思想
* 原始特征是数据点的描述
* 使用算法来创建一组新的特征
* 用更少的列来描述数据点，并且效果不变

### 特点
* 使用数据集中隐藏的结构，可以生成捕获数据本质的新特征
* 使用每个列的一点点特征，创建超级列
* 可以使用更少的特征来描述数据集


### 主成分分析（PCA）
#### 特点
* 无监督的算法
* 捕获整个数据的方差
* 可以消除相关特征，所有的主成分都是线性无关的

#### 原理
* 使用了协方差矩阵的特征值分解

#### 步骤
1. 创建数据集的协方差矩阵
1. 计算该矩阵的特征值
1. 保留前n个特征值，组成特征向量（按解释方差的百分比降序排列）
1. 使用该向量转换原始数据集集


### 线性判别分析（LDA）
#### 特点
* 有监督的算法
* 可以消除相关特征
* 通过响应变量来捕获类别的可分性

#### 原理
* 使用了类内、类间的散布矩阵

#### 步骤
1. 计算每个类别的均值向量
1. 计算类内、类间的散布矩阵
1. 合并两个散布矩阵
1. 计算合并矩阵的特征值，特征向量
1. 降序排列特征值，保留前k个特征向量
1. 使用特征向量转换原始数据集



## 特征学习

### 参数假设
* 定义：算法对数据形状有基本假设
* 例子：PCA，LDA

### 特点
* 依赖于随机学习，不对数据的形状有任何假设
* 一般都使用深度学习


### 受限玻尔兹曼机（RBM）
#### 特点
* 无监督算法
* 使用概率模型来学习新特征
* 一个浅层（两层）的神经网络

#### 原理
* 网络包含可见层、隐藏层
* 一次迭代包括：前向传导、反向激活、调整权重
* 多次迭代后，最小化原始输入和近似值的距离
* 前向传导生成激活变量（权重和偏差）
* 反向激活生成重建原始输入（相同的权重和不同的偏差）


### 词嵌入
#### 特点
* 帮助机器理解上下文的一种方法
* 单词在n维特征空间的向量化，每个单词会对应一个n维向量

#### Word2Vec
* 使用浅层神经网络实现
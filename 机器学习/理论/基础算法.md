

# 基础算法

[TOC]



## 相似度计算

### 欧式距离
#### 特点
* 使用距离来表示相似度
* 结果是一个绝对的客观值
* 适用于分析用户能力模型之间的差异：消费能力、贡献内容

#### 定义
* 在欧式空间下的度量的距离

#### 公式
```math
E = \sqrt{ \sum_{i=1}^{n} ( x_{i} - y_{i} )^2 } 
```

#### 值
* 范围：0 -- 正无穷
* 通常需要转化为 [-1, 1] 或者 [0, 1]



### 余弦相似度
#### 特点
* 使用向量夹角来表示相似度
* 值与向量的长度无关，经过了向量长度归一化处理
* 适用于测量文本相似度、用户相似度、物品相似度

#### 定义
* 取向量夹角的余弦值

#### 公式
##### 一般形式
```math
\cos(\Theta) = \frac{ \sum_{i=1}^{n}(x_i \times y_i) }
                    { \sqrt{\sum_{i=1}^{n}(x_i)^2}
                      \times  \sqrt{\sum_{i=1}^{n}(y_i)^2} } 
```

##### 矩阵形式
```math
\cos(\Theta) = \frac{ a \bullet b }
                    { \left \| a \right \| 
                       \times \left \| b \right \| } 
```

#### 值
* 范围：-1 -- 1
* 1 表示夹角为0度，0表示夹角为90度，-1表示夹角为180度


### 皮尔逊相关系数
#### 特点
* 一种改进后的余弦相似度
* 对向量做了中心化处理（向量减去向量均值）
* 适用于测试两个向量的变化趋势是否一致
* 不适用于计算布尔值向量

#### 公式
```math
R(p,q) = \frac{ \sum_{i=1}^{n}((p_i - \bar{p}) (q_i - \bar{q})) }
                    { \sqrt{\sum_{i=1}^{n}(p_i - \bar{p})^2}
                      \times  \sqrt{\sum_{i=1}^{n}(q_i - \bar{q})^2} }
```

#### 值
* 范围：-1 -- 1
* -1 表示负相关，1表示正相关


### Jaccard相似度
#### 特点
* 适用于计算布尔向量
* 适用于隐式反馈数据

#### 定义
* 两个集合中，交集元素个数占并集元素个数的比例

#### 公式
##### 一般形式
```math
J(p,q)= \frac{ \sum_{i=0}^{n} p_i q_i  }
             { \sum_{i=0}^{n} p_i \mid q_i } 
```

##### 集合形式
```math
J(p,q)= \frac{ \left | P \cap Q  \right | }
             { \left | P \cup  Q  \right | } 
```


## 平滑处理

### 作用
* 解决零概率问题
* 计算测试样本的概率时，如果某个分量在训练集中未出现过，导致整个样本的概率为0

### 思想
* 提高低概率，降低高概率，尽量使概率分布趋于均匀

### 方案
#### 拉普拉斯平滑（加一平滑）
* 方法：计算概率时，分子加一，分母加K（类别数目）

##### 公式
```math
g(x) = \frac{ n_x + \alpha  }{ total + \alpha c } 
```
说明
* n为x类别的次数
* total为总次数
* c为类别数目
* $\alpha$为平滑系数，一般取1，值越大结果越接近均匀分布




## 参数估计

### 最大似然估计
#### 原理
* 利用给定的样本数据，从所有参数中找出能最大概率生成样本的参数
* 基于概率论的极大似然原理

#### 前提
* 训练样本的分布能代表真实分布
* 所有样本都满足独立同分布
* 有充分的训练样本

#### 概念
* 概率：在分布函数参数已知时，出现观测结果的可能性大小
* 似然：已知观测结果时，分布函数的参数为某值的可能性大小


#### 步骤
1. 写出似然函数
1. 对似然函数取对数
1. 求导数
1. 解似然方程


#### 似然函数
##### 公式
$$
l(\theta) = p(D|\theta) = \prod_{i=1}^{N}p(x_i|\theta) 
$$

##### 说明
* $\theta$ 为参数向量
* D 为样本集 ${x_1, x_2, ..., x_N}$
* $p(D|\theta)$ 为联合概率密度函数，因为样本之间独立，可以转换成乘法模型

#### 求解
##### 公式
$$
\hat{\theta}  = \arg \underset{\theta}{max}  \sum_{i=1}^{N} \ln p(x_i|\theta)
$$

##### 说明
* 为了方便求解，一般会使用对数似然函数
* $ H(\theta) = \ln{l(\theta)} $



### EM算法
#### 概述
* 全名：期望最大化算法（Expectation-Maximum）
* 使用了启发式的迭代方法
* 用途：有隐变量的概率模型参数的极大似然估计
* 两个步骤
    1. E步，求期望
    1. M步，求极大

#### 应用
* 支持向量机的SMO算法
* 混合高斯模型
* K-means
* 隐马尔可夫模型

#### 特点
* 结果与初值的选择有关

#### 思路
1. 猜测隐含数据
1. 基于观察数据和猜测的隐含数据一起极大化对数似然
1. 不停迭代步骤1，2，直到参数基本无变化

#### 流程
1. 随机初始化模型参数的初值$\theta^0$
1. for j from 1 to J
    * E步：计算联合分布的条件概率期望(Q函数为核心)
        $$ Q(\theta, \theta^j)= 
           \sum_{i=1}^{m} \sum_{z^i} 
           P(z^i|x^i, \theta^j) log P(x^i, z^i | \theta) 
        $$
    * M步：极大化条件概率期望，得到$\theta^{j+1}$
        $$ \theta^{j+1} = \arg \max_{\theta} Q(\theta, \theta^j) $$
1. 重复步骤E，M，直到$\theta$收敛

##### 说明
* $x$为样本数据
* $z$为隐含数据
* $p(x, z | \theta)$为联合分布
* $p(z | x, \theta)$为条件分布




### 最小二乘法
#### 用途
* 函数拟合
* 求函数极值

#### 原理
* 最小目标函数

#### 公式
```math
target = \sum (observed - expected)^2
```

* target 目标函数，需要使其最小化
* observed 观测值，即样本数据
* expected 理论值，通过拟合函数获得的值

#### 解法
1. 求偏导数
1. 令偏导数为0
1. 求解方程组

#### 缺点
* 需要求解逆矩阵，有时候可能不存在
* 当样本特征非常多时，计算逆矩阵非常耗时
* 需要拟合函数是线性的




## 优化算法

### 梯度下降法
#### 思路
* 每次沿着梯度方向，逐步迭代的求得最优解

#### 梯度
* 定义：各个参数偏导数的向量形式
* 几何意义：函数变化增加最快的地方

#### 概念
* 步长（learning rate）：决定每一步沿梯度负方向前进的长度
* 假设函数：监督学习中，用于拟合输入样本
* 损失函数（loss function）：用于度量拟合的好坏程度，值越小，拟合越好

#### 过程
* 确定模型的假设函数和损失函数
* 初始化相关参数：步长、终止距离、参数初始值
* 迭代
    1. 计算当前参数的损失函数的梯度
    2. 计算当前下降的距离：步长 乘以 梯度
    3. 确定对于所有参数，下降距离是否小于终止距离
    4. 更新所有参数：新参数值 = 旧参数值 - 下降距离
    5. 重复1，2，3，4步骤


#### 调优
##### 步长
###### 问题
* 值太小，使得迭代速度太慢
* 值太大，出现不平稳的现象，无法达到收敛状态

###### 选择方法
* 从较小的学习率开始尝试
* 如果出现不平稳现象，就调小学习率

##### 归一化
###### 问题
* 如果不同特征的取值范围不一样，导致迭代变慢
* 方法：对每个特征数据进行 z分数标准化


#### 策略（如何选择样本）
##### 批量梯度下降（BGD）
* 方法：每次更新参数时，选择所有样本来计算下降距离
* 缺点：计算量比较大，速度慢

##### 随机梯度下降（SGD）
* 方法：每次更新参数时， 随机选择一个样本来计算下降距离
* 缺点：收敛速度比较慢

##### 小批量梯度下降（MBGD）
* 方法：每次更新参数时，选择一组样本来计算下降距离


### 坐标下降法
#### 思路
* 每次沿着单个维度方向进行搜索
* 当得到当前维度方向的最小值后，循环使用不同的维度方向

#### 优点
* 将一个高维优化问题，分成多个一维优化问题。降低了复杂度





## 随机采样算法

### MCMC
#### 概述
* 全称：马尔科夫链蒙特卡罗
* 是很多复杂算法的求解基础

#### 如何整合
##### 结合
* 蒙特卡罗方法（需要大量采样）
* 马尔可夫链在平稳状态下，随机游走一次等于采样一次（可以大量采样）

##### 相等
* 采样的目标分布$p(x)$
* 马尔可夫的平稳分布

#### 基本步骤
1. 构造一个马尔可夫链，使其平稳分布为目标分布$p(x)$
1. 从初始状态$x_0$出发，用马尔可夫链随机游走，产生样本序列
1. 应用遍历定理，确定正整数$m$和$n$，得的平稳分布的样本集合
    $\{x_{m+1},x_{m+2},...,x_{n}\}$
1. 求得函数均值
    $$ \hat{E}f = \frac{1}{n - m} \sum_{i=m+1}^n f(x_i) $$





## 其他

### 奇异值分解(SVD)
#### 性质
* 奇异值减少的特别快
* 一个很大的矩阵可以用三个小的矩阵来表示

#### 应用
* 用于降维算法
* 用于推荐系统
* 用于NLP

#### 特征分解
$$ A = W \Sigma W^T $$
说明
* $W$为 $A$的$n$个特征向量标准化后组成的矩阵
    * 维度 $n \times n$
    * 满足 $ W^T W = I $ （称为 酉矩阵）
* $\Sigma$为 $A$的$n$个特征值为主对角线的矩阵
    * 维度 $n \times n$

##### 特点
* 只能对方阵进行分解

##### 特征值和特征向量
$$ A x = \lambda x $$
说明
* $A$为 $n \times n$的实对称矩阵
* $\lambda$为 矩阵$A$的一个特征值
* $x$为 一个$n$维向量，特征值对应的特征向量


#### 定义
$$ A = U \Sigma V^T $$
说明
* $A$为 $m \times n$的矩阵
* $U$为 $m \times m$的矩阵
    * 满足 $ U^T U = I $ 
* $V$为 $n \times n$的矩阵
    * 满足 $ V^T V = I $ 
* $\Sigma$为 $m \times n$的矩阵
    * 主对角线上的元素为奇异值
    * 非主对角线的元素都为0

##### 特点
* SVD可以对非方阵的矩阵进行分解


#### 计算过程
##### 求右奇异向量
* 将$A$的转置和$A$做矩阵乘法
* 对方阵进行特征分解
* 将所有特征向量组成矩阵$V$
    * 维度 $n \times n$

###### 公式
$$ (A^T A) v_i = \lambda_i v_i $$

##### 求左奇异向量
* 将$A$和$A$的转置做矩阵乘法
* 对方阵进行特征分解
* 将所有特征向量组成矩阵$U$
    * 维度 $m \times m$

###### 公式
$$ (A A^T) u_i = \lambda_i u_i $$

##### 求奇异值矩阵
$$ \sigma_i = \frac{A v_i}{u_i} $$
说明
* $\sigma_i$ 为奇异值
* 可以由 $A = U \Sigma V^T$ 推导出

$$ \sigma_i = \sqrt{\lambda_i} $$
说明
* $\lambda$为 $A^T A$的特征值





## 参考资料
* [人人都懂EM算法](https://zhuanlan.zhihu.com/p/36331115)
* [EM算法原理总结](https://www.cnblogs.com/pinard/p/6912636.html)
* [极大似然估计原理](https://blog.csdn.net/qq_41554005/article/details/100579628!)


# 神经网络

[TOC]


## 简介

### 重要概念
#### 神经元模型
* M-P神经元模型
* 接收来自n个其他神经元的输入信号
* 输入信号通过带权重的连接进行传递
* 接收到的总输入与阈值进行比较
* 通过**激活函数**处理产生输出

#### 感知机
* 看成由两层神经元组成：输入层、输出层
* 输入层接收外部信号后直接传递给输出层
* 输出层是 M-P神经元
* 只有一层功能神经元，学习能力有限

#### 多层前馈神经网络
* 每层神经元与下一层神经元全互连
* 神经元之间不存在同层连接，也不存在跨层连接
* 输入层只接受输入，隐藏层和输出层包含功能神经元
* **前馈**：拓扑结构上不存在环或者回路


### 误差逆传播算法（BP算法）
#### 流程
1. 将样本提供给输入层，逐层将信号前传，产生输出层结果
1. 计算输出层的误差，将误差逆向传播到隐藏层
1. 根据隐藏层神经元的误差来调整神经元的连接权重和阈值
1. 迭代进行步骤1，2，3 直到达到停止条件为止

#### 缓解过拟合
* 早停：将数据分成训练集和验证集，若训练集误差降低但验证集误差升高，则停止训练
* 正则化：在误差目标函数中增加用于描述神经网络复杂度的部分

#### 避免局部极小
* 使用多组初始化值来训练神经网络，取其中误差最小的解作为最终参数
* 模拟退火：在每一步都以一定概率接受比当前最优解更差的结果
* 随机梯度下降：在计算梯度时加入随机因素


### 常见神经网络
#### RBF(径向基函数网络)


## 深度神经网络（DNN）

### 模型
#### 概述
* 在感知机模型扩展而来
* 将很多了神经单元连接为网络状

#### 神经元
##### 定义
* 多个输入进过一个线性关系，得到一个加权值
    * $ z = w_1 x_1 + w_2 x_2 + ... + w_n x_n + b $
* 加权值经过一个激活函数处理，得到输出

##### 图示
![](https://gitee.com/cc12703/figurebed/raw/master/img/20201225192424.png)

#### 基本结构
##### 定义
* 分成三类不同的层：输入层(一个)、隐藏层(多个)，输出层(一个)
* 层与层之间是全连接的：前一层的神经单元与下一层的所有神经单元都有连接

##### 图示
![](https://gitee.com/cc12703/figurebed/raw/master/img/20201225194007.png)


### 数学模型
#### 符号定义
| 符号 | 含义 | 
| -- | -- | 
| $x_i$ | 输入层的第i个神经元的输入 |
| $a_j^l$ | 第l层的第j个神经元的输出 |
| $z_j^l$ | 第l层的第j个神经元的加权值 |
| $b_j^l$ | 第l层的第j个神经元的偏置 |
| $w_{ji}^l$ | 第l-1层的第i个神经元到第l层的第j个神经元的输入权重 |

#### 图示
![](https://gitee.com/cc12703/figurebed/raw/master/img/20201225201751.png)


### 前向传播算法
* 利用上一层输出计算下一层的输出

#### 公式
$$
a^l = \sigma(W^l a^{l-1} + b^l)
$$
说明
* $W$为矩阵
* $b$, $a$为向量
* $\sigma$为激活函数

#### 流程
* 输入：总层数L，权重矩阵W，偏置向量b，输入向量x
* 输出：输出层的输出 $a^L$
* 步骤
    1. 初始化 $a^1 = x$
    2. for l=2 to L，计算$a^l$


### 反向传播算法
* 用于梯度下降法对损失函数求极小值
* 利用下一层的神经单元误差，来计算上一层的神经单元误差

#### 神经单元误差
$$
\delta_j^l = \frac{\partial C}{\partial z_j^l}
$$

#### 公式
$$
\delta^l = (W^{l+1})^T \delta^{l+1} \odot \sigma'(z^l)
$$

#### 梯度公式
$$
\frac{\partial C}{\partial w_{ji}^l} = \delta_j^l a_i^{l-1}
$$
$$
\frac{\partial C}{\partial b_j^l} = \delta_j^l
$$

#### 流程
* 输入：
    * 总层数L，总神经元个数，激活函数，损失函数，输入的m个样本
    * 迭代步长$\alpha$，最大迭代次数MAX，停止迭代阈值$\epsilon$
* 输出：
    * 权重矩阵$W$，偏置向量$b$
* 步骤
    1. 初始化$W$,$b$为随机值
    1. for iter to 1 to MAX
        1. 计算出各层的神经单元误差
            1. for i = 1 to m
            1. 设置 $a^1 = x$
            1. for l = 2 to L : 计算 $a_i^l$ 的值
            1. 通过损失函数，计算出输出层的 $\delta^L$
            1. for l = L-1 to 2 : 计算各层的 $\delta^l$值
        1. 更新权重、偏置
            1. for 𝑙 = 2 to L
            1. $ W^l = W^l - \alpha \delta^l (a^{l-1})^T $
            1. $ b^l = b^l -  \alpha \delta^l $
    1. 如果所有的$W$,$b$变化值都小于$\epsilon$，则跳出循环



### 函数选择
* 损失函数 + 激活函数

#### 组合1
* 均方差 + Sigmoid
* Sigmoid函数会导致算法收敛速度慢

#### 组合2
* 交叉熵 + Sigmoid
* 加快算法收敛速度

#### 组合3
* 对数似然 + softmax
* 处理多分类问题


## 卷积神经网络（CNN）

### 基本结构
* 隐藏层由多个具有结构的层组成
* 结构：卷积层 和 池化层

#### 卷积层
* 使用过滤器扫描原始数据，进行卷积操作
* 卷积结果称为 特征映射

#### 池化层
* 通过池化进行信息压缩
* 最常用的方法：最大池化，选择区域中的最大值


### 数学模型
#### 符号定义
| 符号 | 含义 | 
| -- | -- | 
| $x_{ij}$ | 输入层中神经元的输入值（第i行，第j列）|
| $w_{ij}^{F_k}$ | 建立第k个特征映射过滤器的第i行j列的值 |
| $z_{ij}^{F_k}$ | 卷积层第k个子层，第i行j列神经元的加权输入|
| $b^{F_k}$ | 卷积层第k个子层，第i行j列神经元的偏置 |
| $a_{ij}^{F_k}$ | 卷积层第k个子层，第i行j列神经元的输出 |
| $z_{ij}^{P_k}$| 池化层第k个子层，第i行j列神经元的输入 |
| $a_{ij}^{P_k}$ | 池化层第k个子层，第i行j列神经元的输出 |
| $w_{k-ij}^{O_n}$| 池化层第k个子层，第i行j列神经元指向输出层第n个神经元的权重|
| $z_n^o$| 输出层第n个神经元的加权输入|
| $b_n^o$| 输出层第n个神经元的偏置|
| $a_n^o$| 输出层第n个神经元的输出|

#### 关系图
![](https://gitee.com/cc12703/figurebed/raw/master/img/cnn_struct.png)


### 前向传播算法
#### 传播到卷积层
$$
a^l = \sigma(a^{l-1} * W^l + b^l)
$$
说明
* $*$表示卷积操作
* $\sigma$表示激活函数，一般使用ReLU

#### 传播到池化层
$$ a^l = pool(a^{l-1}) $$

### 反向传播算法
#### 池化层
$$ \delta^{l-1} = upsample(\delta^l) \odot \sigma' z^{l-1} $$

##### upsample操作
* 对于池化操作max，将值放置到原来的位置上
* 对于池化操作Average，将值取平均值，依次放置在原来位置上

###### upsample-max
$$
\begin{pmatrix}
  2 &8 \\
  4 &6
\end{pmatrix}
=>
\begin{pmatrix}
  2 &0  &0  &0 \\
  0 &0  &0  &8 \\
  0 &4  &0  &0 \\
  0 &0  &6  &0
\end{pmatrix}
$$

###### upsample-average
$$
\begin{pmatrix}
  2 &8 \\
  4 &6
\end{pmatrix}
=>
\begin{pmatrix}
  0.5 &0.5  &2  &2 \\
  0.5 &0.5  &2  &2 \\
  1 &1  &1.5  &1.5 \\
  1 &1  &1.5  &1.5
\end{pmatrix}
$$

#### 卷积层
$$ 
\delta^{l-1} = \delta^l * rot180(W^l) \odot \sigma' z^{l-1}
$$
说明
* $rot180$表示卷积核被旋转了180度（上下翻转一次，左右翻转一次）





## 参考资料
* https://www.cnblogs.com/pinard/p/6418668.html
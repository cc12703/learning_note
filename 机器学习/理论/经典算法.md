

# 经典算法

[TOC]


## k最近邻（KNN）

### 概述
* 一种监督式学习算法

### 优点
* 思想简单，也用于回归和分类
* 用于非线性分类
* 训练的时间复杂度比较低
* 对数据没有假设，对异常点不敏感

### 缺点
* 预测计算量大，尤其特征数非常多的时候
* 训练是需要大量内存
* 样本不平衡时，对稀有类型的预测准确率低
* 可解释性不强

### 适用情况
* 样本容量比较大的类域的自动分类

### 要点
#### k值选择
##### 方法
* 选择一个较小的值，通过交叉验证选择一个合适的值

##### 值小
* 训练误差小，泛化误差大
* 模型整体变复杂，容易发生过拟合

##### 值大
* 泛化误差小，训练误差大
* 模型整体变简单，容易预测错误

#### 距离度量
* 欧式距离（最常用）
* 曼哈顿距离
* 闵可夫斯基距离（Minkowski）

#### 决策规则
* 回归：平均法，输出最近的k个样本的平均值
* 分类：多数表决法，选最近的k个样本数量最多的那个类别


### 实现方案
#### KD树
##### 原理
* 通过建立KD树，来减少无效的最近邻搜索

##### 步骤
1. 建树
1. 搜索最近邻
1. 预测

#### 球树（BallTree）
* 进一步优化KD树的搜索效率


### 扩展
* 限定半径最近邻：解决样本中某类别的样本非常少的情况
* 质心算法：通常用于文本分类





## 决策树

### 概述
#### 优点
* 原理简单，可用于回归、也可用于分类
* 数据基本不需要预处理
* 预测计算量小
* 既可以处理离散值，也可以处理连续值
* 解释性强
* 对异常点不敏感

#### 缺点
* 非常容易过拟合
* 结果容易随样本改动而剧烈变动
* 容易陷入局部最优
* 若某特征的样本比例过大，生成的树容易偏向这些特征


### 树组成
* 包括：一个根节点、若干中间节点、若干叶节点

#### 根节点
* 包含所有样本

#### 中间节点
* 每个节点对应一个属性测试
* 节点包含的样本会根据属性测试的结果划分到子节点中

#### 叶节点
* 包含决策结果


### 构建-划分选择
#### 目标
* 节点所包含的样本尽可能属于同一类别

#### 衡量标准
##### 信息增益
* 定义：按属性分类后，信息熵值的下降程度
* 选择：选择值最大的属性作为最优划分属性
* 公式
    $$
Gain(D,a) = Ent(D) - \sum_{v=1}^{V} 
                     \frac{|D^v|}{|D|}
                     Ent(D^v) 
    $$
    * D: 全样本集
    * $D^v$ 包含特定属性值的样本集
    * a: 属性
    * V: 属性a的可能取值
    * Ent() 集合的信息熵
    * |D| 集合的样本数
* 缺点
    * 对可取值数目较多的属性有所偏好

##### 增益率
* 选择：选择值最大的作为最后划分属性
* 公式
    $$
Gain\_ratio(D,a) = \frac{ Gain(D,a) }
                 { - \sum_{v=1}^{V} 
                          \frac{|D^v|}{|D|} log_2(\frac{|D^v|}{|D|})
                  } 
    $$
* 缺点
    * 对可取值数目比较少的属性有所偏好

##### 基尼指数
* 定义：按属性分类后子集合基尼数的加权和
* 选择：选择值最小的属性作为最优划分属性
* 公式
    $$
Gini\_index(D,a) = \sum_{v=1}^{V} 
                          \frac{|D^v|}{|D|} Gini(D^v) 
    $$
    * Gini() 集合的基尼值


### 剪枝
#### 目的
* 防止决策树过拟合

#### 策略
##### 预剪枝
* 原理：在构造树的同时进行剪枝
* 过程
    * 每个节点在划分前先进行泛化能力评估
    * 若划分不能带来泛化能力的提升，则停止划分并将其标记为叶节点
* 泛化能力评估：预留一部分数据作为验证集
* 优点
    * 减少训练时间开销
* 缺点
    * 有欠拟合的风险

##### 后剪枝
* 原理：在构造一棵完整的树后，再进行剪枝
* 过程
    * 生成一棵完整的树
    * 从底向上对中间节点进行泛化能力评估
    * 若去除该节点会带来性能提升，则将其替换为叶节点
* 优点
    * 欠拟合风险比较小
* 缺点
    * 训练时间开销比较大


### 连续值
* 需要对连续值进行离散处理


### 实现方案
#### ID3
##### 特点
* 只支持分类任务
* 使用多叉树
* 使用 信息增益 选择特征
* 不支持连续值特征
* 不支持缺失值
* 不支持剪枝

##### 缺点
* 容易选择取值比较多的特征
* 容易过拟合


#### C4.5
* ID3的改进版本

##### 特点
* 只支持分类任务
* 使用多叉树
* 使用 信息增益比 选择特征
* 支持连续值
* 支持缺失值
* 支持剪枝

##### 改进点
* 将连续特征离散化
* 使用正则化系数进行剪枝操作


#### CART
##### 特点
* 支持分类、回归任务
* 使用二叉树
* 使用基尼系数、均方差 选择特征
* 支持连续值
* 支持缺失值
* 支持剪枝

##### 树
###### 回归树
* 样本输出是连续值
* 使用均 方差 选择特征
* 使用叶子节点的均值或中位数来预测结果

###### 分类树
* 样本输出是离散值
* 使用 基尼系数 选择特征
* 使用叶子节点中概率最大的类别作为预测类别



### 注意点
#### 若样本少，特征非常多
* 决策树容易过拟合
* 推荐先使用降维算法压缩特征

#### 多用可视化来观察数据的拟合情况
* 可以先限制树的深度

#### 若样本是稀疏的
* 拟合前转为csc格式
* 预测前转为csr格式
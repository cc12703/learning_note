

# 经典算法

[TOC]


## k最近邻（KNN）

### 概述
* 一种监督式学习算法

### 优点
* 思想简单，也用于回归和分类
* 用于非线性分类
* 训练的时间复杂度比较低
* 对数据没有假设，对异常点不敏感

### 缺点
* 预测计算量大，尤其特征数非常多的时候
* 训练是需要大量内存
* 样本不平衡时，对稀有类型的预测准确率低
* 可解释性不强

### 适用情况
* 样本容量比较大的类域的自动分类

### 要点
#### k值选择
##### 方法
* 选择一个较小的值，通过交叉验证选择一个合适的值

##### 值小
* 训练误差小，泛化误差大
* 模型整体变复杂，容易发生过拟合

##### 值大
* 泛化误差小，训练误差大
* 模型整体变简单，容易预测错误

#### 距离度量
* 欧式距离（最常用）
* 曼哈顿距离
* 闵可夫斯基距离（Minkowski）

#### 决策规则
* 回归：平均法，输出最近的k个样本的平均值
* 分类：多数表决法，选最近的k个样本数量最多的那个类别


### 实现方案
#### KD树
##### 原理
* 通过建立KD树，来减少无效的最近邻搜索

##### 步骤
1. 建树
1. 搜索最近邻
1. 预测

#### 球树（BallTree）
* 进一步优化KD树的搜索效率


### 扩展
* 限定半径最近邻：解决样本中某类别的样本非常少的情况
* 质心算法：通常用于文本分类





## 决策树

### 概述
#### 优点
* 原理简单，可用于回归、也可用于分类
* 数据基本不需要预处理
* 预测计算量小
* 既可以处理离散值，也可以处理连续值
* 解释性强
* 对异常点不敏感

#### 缺点
* 非常容易过拟合
* 结果容易随样本改动而剧烈变动
* 容易陷入局部最优
* 若某特征的样本比例过大，生成的树容易偏向这些特征

#### 模型结构
##### 组成
* 一个根节点：包含所有样本
* 若干个内部节点：对应一个属性测试
    * 节点包含的样本会根据属性测试的结果划分到子节点中
* 若干个叶节点：代表决策结果

##### 图示
![](https://gitee.com/cc12703/figurebed/raw/master/img/20210113173253.png)

#### 步骤
1. 特征选择：决定使用哪些特征来做判断
1. 树生成：从根节点出发，根据选择好的特征建立子节点，用训练集递归生成
1. 树剪枝：主动去掉部分分支来减少“过拟合”


#### 类型
##### 分类树
* 样本输出是离散值
* 预测时，使用叶子节点中概率最大的类别作为预测类别 

##### 回归树
* 样本输出是连续值
* 预测时，使用叶子节点的均值或中位数来作为预测结果

#### 算法
##### ID3
* 内部使用多叉树
* 使用信息增益来选择特征

##### C4.5
* ID3的改进算法
* 使用信息增益率来选择特征

##### CART
* 内部使用二叉树
* 使用基尼系数来选择特征
* 即可用于分类、也可以用于回归


### 特征选择标准
#### 目标
* 节点所包含的样本尽可能属于同一类别

#### 信息增益
* 定义：按属性分类后，信息熵值的下降程度
* 选择：选择值最大的属性作为最优划分属性
* 公式
    $$
Gain(D,a) = Ent(D) - \sum_{v=1}^{V} 
                     \frac{|D^v|}{|D|}
                     Ent(D^v) 
    $$
    * D: 全样本集
    * $D^v$ 包含特定属性值的样本集
    * a: 属性
    * V: 属性a的可能取值
    * Ent() 集合的信息熵
    * |D| 集合的样本数
* 缺点
    * 对可取值数目较多的属性有所偏好

#### 增益率
* 选择：选择值最大的作为最后划分属性
* 公式
    $$
Gain\_ratio(D,a) = \frac{ Gain(D,a) }
                 { - \sum_{v=1}^{V} 
                          \frac{|D^v|}{|D|} log_2(\frac{|D^v|}{|D|})
                  } 
    $$
* 缺点
    * 对可取值数目比较少的属性有所偏好

#### 基尼指数
* 定义：按属性分类后子集合基尼数的加权和
* 选择：选择值最小的属性作为最优划分属性
* 公式
    $$
Gini\_index(D,a) = \sum_{v=1}^{V} 
                          \frac{|D^v|}{|D|} Gini(D^v) 
    $$
    * Gini() 集合的基尼值



### 剪枝
#### 目的
* 防止决策树过拟合

#### 预剪枝
* 原理: 在构造树的同时进行剪枝
* 过程
    1. 每个节点在划分前先进行泛化能力评估
    1. 若划分不能带来泛化能力的提升，则停止划分并将其标记为叶节点
* 优点: 减少训练时间开销
* 缺点: 有欠拟合的风险

#### 后剪枝
* 原理: 在构造一棵完整的树后，再进行剪枝
* 过程
    1. 生成一棵完整的树
    1. 从底向上对中间节点进行泛化能力评估
    1. 若去除该节点会带来性能提升，则将其替换为叶节点
* 优点: 欠拟合风险比较小
* 缺点: 训练时间开销比较大



### CART
#### 特点
* 支持分类、回归任务
* 使用二叉树
* 使用基尼系数、均方差 选择特征
* 支持连续值
* 支持缺失值
* 支持剪枝


### 构建-分类树
#### 流程
1. 判断是否停止递归
    * 当前节点的样本集中，样本个数小于阈值、或者没有特征
    * 样本集的基尼系数小于阈值
1. 计算当前节点各个特征的各个特征值对样本集的基尼系数
1. 在所有计算结果中，选择基尼系数最小的特征和对应的特征值
1. 更加选出的最优特征和特征值，将样本集分成两部分，作为当前节点的左右子节点
1. 对左右子节点递归调用步骤1--4


#### 连续值处理
##### 方法
* 将连续值离散化
* 选择划分点的时，使用基尼系数作为度量方式

##### 思路
1. 将连续特征值从小到大排列（假定一共m个）
1. 取相邻值的平均值作为划分点（一个m-1个）
1. 对于划分点，分别计算以该点作为二元分类点的基尼系数
1. 选择基尼系数最小的划分点作为连续值的二元分类点


#### 离散值处理
##### 方法
* 不停的二分离散值

##### 思路
1. 将离散值分成两个部分：一个特征值，其余特征值
1. 计算所有组合的基尼系数，选基尼系数最小的组合

##### 划分例子
* 对于特征A，有值A1,A2,A3
* 所有组合为 
    * {A1}, {A2,A3}
    * {A2}, {A1,A3}
    * {A3}, {A1,A2}


### 构建-回归树
#### 流程
**和构建-分类树是基本一致，不同点是连续值的处理**

#### 连续值处理
##### 方法
* 选择划分点的时，**和方差**使用作为度量方式

##### 公式
$$
\underbrace{\min}_{A,s} \left[ 
    \underbrace{\min}_{c_1} \sum_{x_i \in D_1} (y_i - c_1)^2
    + \underbrace{\min}_{c_2} \sum_{x_i \in D_2} (y_i - c_2)^2
\right]
$$
说明
* $A$为特征，$s$为划分点
* $D_1, D_2$是划分成的两个数据集
* $c_1, c_2$是两个数据集的样本输出均值

含义
* 该划分点要使两个集合各自的均均方差最小，同时要两个集合的均方差之和也最小


### 缺失值处理
#### 问题
* 在样本某些特征缺失的情况下，选择划分的值
* 在选定划分值后，如何处理缺失特征的样本

#### 选择划分值

#### 处理样本
* 将缺失特征的演变同时划入所有的子节点
* 样本权重需要按子节点样本的数量比例来分配


### 剪枝
#### 概述
* 使用后剪枝法
* 分类树和回归树算法基本一样，除了度量损失不一样
* 度量损失
    * 回归树使用 均方差
    * 分类树使用 基尼系数
* 步骤
    1. 从原始决策树生成各种剪枝效果的决策树
    1. 使用交叉验证来检验剪枝后的预测能力
    1. 选择泛化能力最好的剪枝树作为CART树

#### 损失函数
$$ C_{\alpha}(T_t) = C(T_t) + \alpha |T| $$
说明
* $\alpha$为正则化参数
* $C(T_t)$为训练数据的预测误差
* $|T_t|$为子树的叶子节点数量

#### 流程
1. 初始化变量
    * $\alpha_{min} = \infty$, $k = 0$
    * $T$为原始决策树
    * 最优子树集合 $\omega = \{T\}$
1. 从叶子节点开始，自下而上的计算各个内部节点的数据
    * 训练数据的预测误差 $C(T_t)$
    * 叶子节点个数 $|T_t|$ 
    * 正则化阈值 $$ \alpha = \min \{ \frac{C(T) - C(T_t)}{|T_t| - 1}, \alpha_{min} \} $$
    * 更新 $\alpha_{min} = \alpha$
1. $ \alpha_k = \alpha_min$
1. 自上而下的访问子树t的内部节点，如果$\frac{C(T) - C(T_t)}{|T_t| - 1} \le \alpha_k$，则进行剪枝
    * 决策叶节点t的值：分类树使用概率最高的类别，回归树使用样本输出的均值
    * 得到$\alpha_k$对应的最优子树$T_k$
1. 更新集合 $\omega = \omega \cup T_k$
1. 更新 $k = k + 1$, $T = T_k$
1. 如果$T$还有除根节点外的其他节点，则回到步骤2继续执行，否则返回最优子树集合
1. 采用交叉验证在最优子树集合中选择一棵最优子树




## 参考资料
* [CART 分类与回归树](https://www.jianshu.com/p/b90a9ce05b28)
* [决策树算法原理-刘建平](https://www.cnblogs.com/pinard/p/6053344.html)
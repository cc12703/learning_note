

# 降维算法

[TOC]


## 总体

### 作用
* 使数据更容易查看、分类
* 有利于数据压缩

### 要点
* 降维时要最大限度地保留原来的方差



## 主成分分析（PCA）

### 概述
* 一种非监督学习算法
* 假设数据符合高斯分布
* 选择样本投影具有最大方差的方向（分类信息依赖于方差）

### 思想
* 找出数据里最主要的方面，用最主要方面来代替原始数据

### 降维标准
* 基于最小投影距离：样本点到超平面的距离足够近
* 基于最大投影方差：样本点在超平面上的投影尽可能的分开

### 主成分
* 定义：样本集的**协方差矩阵**的前n个**特征值**对应的**特征向量矩阵**

### 步骤
1. 对样本进行中心化
2. 计算样本的协方差矩阵
3. 对矩阵进行特征值分解
4. 取出最大的 n 个特征值对应的特征向量
5. 将所有n个特征向量标准化，生成特征向量矩阵
6. 使用该矩阵对每个样本进行转化，生成新样本

### 优点
* 只需要使用方差信息，不受数据集以外的因素影响
* 各主成分之间正交，可以消除原数据之间相互影响的因素
* 计算简单，主要使用特征值分解




## 线性判别分析（LDA）

### 概述
* 一种监督学习算法
* 假设数据符合高斯分布
* 选择分类性能最好的投影方向（分类信息依赖于均值）

### 思想
* 使投影后的类内方差最小，类间方差最大
* 将数据在低维度上进行投影
* 希望同一种类别数据的投影点尽可能的接近
* 希望不同类别数据的类别中心尽可能的分离

### 步骤
1. 计算类内散度矩阵 Sw
2. 计算类间散度矩阵 Sb
3. 计算出投影矩阵 W
4. 对样本中的每个样本特征，转化为新的样本

### 缺点
* 不适合对非高斯分布样本进行降维
* 最多降维到 k - 1
* 在样本分类信息依赖方差而不是均值时，效果不好
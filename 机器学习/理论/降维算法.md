

# 降维算法

[TOC]


## 总体

### 作用
* 使数据更容易查看、分类
* 有利于数据压缩

### 要点
* 降维时要最大限度地保留原来的方差



## 主成分分析（PCA）

### 概述
#### 特点
* 一种非监督学习算法
* 假设数据符合高斯分布


#### 优点
* 只需要使用方差信息，不受数据集以外的因素影响
* 各主成分之间正交，可以消除原数据之间相互影响的因素
* 计算简单，主要使用特征值分解

#### 缺点
* 特征的解释性变弱
* 会丢失部分信息


### 原理
#### 思想
* 找出数据里最主要的方面，用最主要方面来代替原始数据

### 主成分
* 样本集**协方差矩阵**的前n个**特征值**对应的**特征向量**


### 降维标准
#### 目标
* 希望降维后损失尽可能的小

#### 推导方法
* 基于最小投影距离：样本点到超平面的距离足够近
* 基于最大投影方差：样本点在超平面上的投影尽可能的分开


### 流程
1. 对所有样本进行中心化
    $$ x^i = x^i - \frac{1}{m} \sum_{j=1}^m x^j$$
1. 计算样本的协方差矩阵 $X X^T$
1. 对矩阵进行特征值分解
    $$ X X^T W = \lambda W $$
1. 取出最大的$n'$个特征值对应的特征向量
1. 将所有$n'$个特征向量标准化，生成投影矩阵 $W$
1. 对每个样本进行转换，生成新样本
    $$ z^i = W^T x^i $$

#### 样本集
$D = (x^1, x^2, ..., x^m)$
说明
* $m$为样本个数
* 每个样本为$n$维向量





## 线性判别分析（LDA）

### 概述
#### 特点
* 一种监督学习算法
* 假设数据符合高斯分布
* 即可用于降维、也可用于分类

#### 缺点
* 不适合对非高斯分布样本进行降维
* 最多降维到 k - 1
* 在样本分类信息依赖方差而不是均值时，效果不好


### 思想
* 使投影后的类内方差最小，类间方差最大
    * 将数据在低维度上进行投影
    * 希望同一种类别数据的投影点尽可能的接近
    * 希望不同类别数据的类别中心尽可能的分离


### 原理
#### 二分类推导
##### 定义
* 数据集为 $(x_1, y_1), (x_2, y_2), ..., (x_m, y_m)$
* $x_i$为n维向量，$y_i$值为0或1
* $\mu_i$为第i类样本的均值向量
* $\Sigma_i$为第i类样本的协方差矩阵
* $\omega$为投影直线向量

##### 优化目标
$$ 
\underbrace{\arg \max}_{\omega} J(\omega) = 
\frac{ || \omega^T \mu_0 - \omega^T - \mu_1 ||_2^2  }
     { \omega^T \Sigma_0 \omega + \omega^T \Sigma_1 \omega }
$$
说明
* $\omega^T \mu_i$ 表示第i类中心点在直线$\omega$上的投影
* $\omega^T \Sigma_I \omega$表示第i类样本在直线$\omega$上投影点的协方差

##### 变形后目标
$$
\underbrace{\arg \max}_{\omega} J(\omega) =
\frac{ \omega^T S_b \omega }{ \omega^T S_\omega \omega }
$$
说明
* 利用广义瑞利商的性质获取最大值

### 流程
1. 计算类内散度矩阵 $S_w$
1. 计算类间散度矩阵 $S_b$
1. 计算矩阵 $S_w^{-1} S_b$
1. 对矩阵进行特征值分解
1. 取出最大的$n'$个特征值对应的特征向量，得到投影矩阵$W$
1. 对每个样本进行转换，生成新样本
    $$ z^i = W^T x^i $$


[TOC]

# 模型与算法

## 概述

### 经典算法
* k最近邻（KNN）
* 决策树

### 概率图算法
* 隐马尔可夫模型（HMM）

### 回归算法
* 线性回归

#### 思路
* 用观测值拟合一条曲线、函数
* 用拟合出的曲线、函数去预测未知值

### 分类算法
* 感知机
* 逻辑回归
* 朴素贝叶斯

### 降维算法
* 主成分分析（PCA）
* 线性判别分析（LDA）

#### 作用
* 使数据更容易查看、分类
* 有利于数据压缩

#### 要点
* 降维时要最大限度地保留原来的方差

### 聚类算法
* k均值（K-means）
* DBSCAN

#### 特点
* 解决问题的效果不如有监督算法
* 无监督导致对结果的评估比较困难
* 不同的算法得到的结果差异比较大
* 同一算法不同参数得到的结果差异也会比较大





## k最近邻（KNN）

### 概述
* 一种监督式学习算法

### 优点
* 思想简单，也用于回归和分类
* 用于非线性分类
* 训练的时间复杂度比较低
* 对数据没有假设，对异常点不敏感

### 缺点
* 预测计算量大，尤其特征数非常多的时候
* 训练是需要大量内存
* 样本不平衡时，对稀有类型的预测准确率低
* 可解释性不强

### 适用情况
* 样本容量比较大的类域的自动分类

### 要点

#### k值选择

##### 方法
* 选择一个较小的值，通过交叉验证选择一个合适的值

##### 值小
* 训练误差小，泛化误差大
* 模型整体变复杂，容易发生过拟合

##### 值大
* 泛化误差小，训练误差大
* 模型整体变简单，容易预测错误

#### 距离度量
* 欧式距离（最常用）
* 曼哈顿距离
* 闵可夫斯基距离（Minkowski）

#### 决策规则
* 回归：平均法，输出最近的k个样本的平均值
* 分类：多数表决法，选最近的k个样本数量最多的那个类别

### 实现方案

#### KD树

##### 原理
* 通过建立KD树，来减少无效的最近邻搜索

##### 步骤
1. 建树
1. 搜索最近邻
1. 预测

#### 球树（BallTree）
* 进一步优化KD树的搜索效率


### 扩展
* 限定半径最近邻：解决样本中某类别的样本非常少的情况
* 质心算法：通常用于文本分类


## 决策树

### 概述

#### 优点
* 原理简单，可用于回归、也可用于分类
* 数据基本不需要预处理
* 预测计算量小
* 既可以处理离散值，也可以处理连续值
* 解释性强
* 对异常点不敏感

#### 缺点
* 非常容易过拟合
* 结果容易随样本改动而剧烈变动
* 容易陷入局部最优
* 若某特征的样本比例过大，生成的树容易偏向这些特征


### 树组成
* 包括：一个根节点、若干中间节点、若干叶节点

#### 根节点
* 包含所有样本

#### 中间节点
* 每个节点对应一个属性测试
* 节点包含的样本会根据属性测试的结果划分到子节点中

#### 叶节点
* 包含决策结果


### 构建-划分选择

#### 目标
* 节点所包含的样本尽可能属于同一类别

#### 衡量标准

##### 信息增益
* 定义：按属性分类后，信息熵值的下降程度
* 选择：选择值最大的属性作为最优划分属性
* 公式
    $$
Gain(D,a) = Ent(D) - \sum_{v=1}^{V} 
                     \frac{|D^v|}{|D|}
                     Ent(D^v) 
    $$
    * D: 全样本集
    * $D^v$ 包含特定属性值的样本集
    * a: 属性
    * V: 属性a的可能取值
    * Ent() 集合的信息熵
    * |D| 集合的样本数
* 缺点
    * 对可取值数目较多的属性有所偏好

##### 增益率
* 选择：选择值最大的作为最后划分属性
* 公式
    $$
Gain\_ratio(D,a) = \frac{ Gain(D,a) }
                 { - \sum_{v=1}^{V} 
                          \frac{|D^v|}{|D|} log_2(\frac{|D^v|}{|D|})
                  } 
    $$
* 缺点
    * 对可取值数目比较少的属性有所偏好

##### 基尼指数
* 定义：按属性分类后子集合基尼数的加权和
* 选择：选择值最小的属性作为最优划分属性
* 公式
    $$
Gini\_index(D,a) = \sum_{v=1}^{V} 
                          \frac{|D^v|}{|D|} Gini(D^v) 
    $$
    * Gini() 集合的基尼值


### 剪枝

#### 目的
* 防止决策树过拟合

#### 策略

##### 预剪枝
* 原理：在构造树的同时进行剪枝
* 过程
    * 每个节点在划分前先进行泛化能力评估
    * 若划分不能带来泛化能力的提升，则停止划分并将其标记为叶节点
* 泛化能力评估：预留一部分数据作为验证集
* 优点
    * 减少训练时间开销
* 缺点
    * 有欠拟合的风险

##### 后剪枝
* 原理：在构造一棵完整的树后，再进行剪枝
* 过程
    * 生成一棵完整的树
    * 从底向上对中间节点进行泛化能力评估
    * 若去除该节点会带来性能提升，则将其替换为叶节点
* 优点
    * 欠拟合风险比较小
* 缺点
    * 训练时间开销比较大


### 连续值
* 需要对连续值进行离散处理


### 实现方案

#### ID3

##### 特点
* 只支持分类任务
* 使用多叉树
* 使用 信息增益 选择特征
* 不支持连续值特征
* 不支持缺失值
* 不支持剪枝

##### 缺点
* 容易选择取值比较多的特征
* 容易过拟合

#### C4.5

* ID3的改进版本

##### 特点
* 只支持分类任务
* 使用多叉树
* 使用 信息增益比 选择特征
* 支持连续值
* 支持缺失值
* 支持剪枝

##### 改进点
* 将连续特征离散化
* 使用正则化系数进行剪枝操作

#### CART

##### 特点
* 支持分类、回归任务
* 使用二叉树
* 使用基尼系数、均方差 选择特征
* 支持连续值
* 支持缺失值
* 支持剪枝

##### 树

###### 回归树
* 样本输出是连续值
* 使用均 方差 选择特征
* 使用叶子节点的均值或中位数来预测结果

###### 分类树
* 样本输出是离散值
* 使用 基尼系数 选择特征
* 使用叶子节点中概率最大的类别作为预测类别


### 注意点

#### 若样本少，特征非常多
* 决策树容易过拟合
* 推荐先使用降维算法压缩特征

#### 多用可视化来观察数据的拟合情况
* 可以先限制树的深度

#### 若样本是稀疏的
* 拟合前转为csc格式
* 预测前转为csr格式





## 朴素贝叶斯

### 要点
* 使用贝叶斯公式
* 假设样本特征之间是独立的

### 模型

#### 贝叶斯公式
$$
P(Y_k|X)= \frac{ P(X|Y_k)P(Y_k) }
               { \sum_k P(X| Y=Y_k)P(Y_k) } 
$$

#### 模型公式
$$
C_{result} =  \underbrace{argmax}_{C_k}
              P(Y = C_k)
              \prod_{j=1}^{n} P( X_j = X_{j}^{(test)} | Y = C_k)
$$

**说明**
* n 样本特征个数
* C 类别值，一共k个

### 步骤
1. 计算Y的先验概率
2. 计算第k个类别下的第j个特征的条件概率
3. 通过模型公式，计算所有的k个条件概率
4. 选择概率最大的作为预测类型

### 优点
* 小规模数据的表现比较好
* 能处理多分类
* 适合增量训练
* 对缺失数据不敏感

### 缺点
* 属性数较多、属性间相关性较大时，分类效果差
* 需要知道先验概率
* 对输入数据的表达形式很敏感

### 改进
* 如果有概率为零的情况，需要进行拉普拉斯平滑处理



## 主成分分析（PCA）

### 概述
* 一种非监督学习算法
* 假设数据符合高斯分布
* 选择样本投影具有最大方差的方向（分类信息依赖于方差）

### 思想
* 找出数据里最主要的方面，用最主要方面来代替原始数据

### 降维标准
* 基于最小投影距离：样本点到超平面的距离足够近
* 基于最大投影方差：样本点在超平面上的投影尽可能的分开

### 主成分
* 定义：样本集的**协方差矩阵**的前n个**特征值**对应的**特征向量矩阵**

### 步骤
1. 对样本进行中心化
2. 计算样本的协方差矩阵
3. 对矩阵进行特征值分解
4. 取出最大的 n 个特征值对应的特征向量
5. 将所有n个特征向量标准化，生成特征向量矩阵
6. 使用该矩阵对每个样本进行转化，生成新样本

### 优点
* 只需要使用方差信息，不受数据集以外的因素影响
* 各主成分之间正交，可以消除原数据之间相互影响的因素
* 计算简单，主要使用特征值分解



## 线性判别分析（LDA）

### 概述
* 一种监督学习算法
* 假设数据符合高斯分布
* 选择分类性能最好的投影方向（分类信息依赖于均值）

### 思想
* 使投影后的类内方差最小，类间方差最大
* 将数据在低维度上进行投影
* 希望同一种类别数据的投影点尽可能的接近
* 希望不同类别数据的类别中心尽可能的分离

### 步骤
1. 计算类内散度矩阵 Sw
2. 计算类间散度矩阵 Sb
3. 计算出投影矩阵 W
4. 对样本中的每个样本特征，转化为新的样本

### 缺点
* 不适合对非高斯分布样本进行降维
* 最多降维到 k - 1
* 在样本分类信息依赖方差而不是均值时，效果不好


## k均值（K-means）

### 特点
* 基于距离计算来聚类
* 基于数据本身的特性，将一组数据划分为多个簇
* 根据数据离哪个中心点最近来决定属于哪个簇

### 步骤

#### 划分
1. 随机选择k个中心点
2. 计算每个数据点到中心点的距离，分配给离其最近的中心点
3. 基于簇重新计算中心点，将中心点移动到簇中所有点的中心
4. 重复步骤2，3，直到中心点不再移动

#### 预测
1. 计算新数据点到每个簇中心点的距离
1. 分配给离数据点最近的簇

### 优点
* 快速、简单
* 效果通常还是不错的
* 可解释性比较强

### 缺点
* k值比较难确定
* 在球形簇上效果比较好，其他类型效果一般

### 注意点
* k的选择：从一个比较小的值开始，逐渐增加直到方差不会有大的改善
* 距离度量：使用欧式距离，余弦相似度
* 局部最小值：进行多次聚类，最后将结果平均
* 标记簇：需要研究数据，找出每个簇的意义
* 数据标准化
    * 一定要对数据进行缩放，以达到标准化
    * 在标准化的数据上效果最好



## DBSCAN

### 特点
* 一种基于密度的聚类算法
* 常用于异常检测

### 概念
* r-邻域：给定对象半径r内的邻域
* 核心点：r-邻域内的数据点
* 边界点：落在核心点邻域内的数据点
* 离群点：即不是核心点，也不是边界点

### 步骤
1. 随机选择一个数据点
2. 使用r半径画园
3. 凡是能被其及其下属圈到的数据点都属于同一类别
4. 若该类型已发展完成，就重复1-3步骤，直到所有数据点都处理过

### 优点
* 可以对任意形状的稠密度数据集进行聚类
* 适合于检测任务，寻找离群点
* 不需要指定类别数目

### 缺点
* 如果样本密度不均匀，效果比较差
* 半径选择比较难，导致的结果差异比较大    



## 隐马尔可夫模型（HMM）

### 概述
* 一种有向图模型
* 一种结构最简单的动态贝叶斯网

### 组成

#### 状态变量（隐变量）
* 是隐藏的，不可被观测的
* 表示第 i 时刻的系统状态，使用 Y 表示

#### 观测变量
* 表示第 i 时刻的观测值，使用 X 表示

#### 依赖关系
* 任一时刻，观测变量的值仅依赖于状态变量
* 第 t 时刻的状态仅依赖于第 t - 1 时刻的状态

#### 参数

##### 状态转移概率
* 定义：模型在各个状态间转移的概率

##### 输出观测概率
* 定义：模型根据当前状态获得各个观测值的概率

##### 初始状态概率
* 定义：模型在初始时各个状态出现的概率

### 观测序列生成
1. 设置t = 1, 根据初始状态概率生成隐藏变量 Yt
2. 根据隐藏变量 Yt 和 输出观测概率，生成观测变量 Xt
3. 根据隐藏变量 Yt 和 状态转移概率，生成隐藏变量 Yt+1
4. 若 t < n， 则 t += 1，进入步骤2









## 相似度计算

### 欧式距离

#### 特点
* 使用距离来表示相似度
* 结果是一个绝对的客观值
* 适用于分析用户能力模型之间的差异：消费能力、贡献内容

#### 定义
* 在欧式空间下的度量的距离

#### 公式
```math
E = \sqrt{ \sum_{i=1}^{n} ( x_{i} - y_{i} )^2 } 
```

#### 值
* 范围：0 -- 正无穷
* 通常需要转化为 [-1, 1] 或者 [0, 1]



### 余弦相似度

#### 特点
* 使用向量夹角来表示相似度
* 值与向量的长度无关，经过了向量长度归一化处理
* 适用于测量文本相似度、用户相似度、物品相似度

#### 定义
* 取向量夹角的余弦值

#### 公式

**一般形式**
```math
\cos(\Theta) = \frac{ \sum_{i=1}^{n}(x_i \times y_i) }
                    { \sqrt{\sum_{i=1}^{n}(x_i)^2}
                      \times  \sqrt{\sum_{i=1}^{n}(y_i)^2} } 
```

**矩阵形式**
```math
\cos(\Theta) = \frac{ a \bullet b }
                    { \left \| a \right \| 
                       \times \left \| b \right \| } 
```

#### 值
* 范围：-1 -- 1
* 1 表示夹角为0度，0表示夹角为90度，-1表示夹角为180度


### 皮尔逊相关系数

#### 特点
* 一种改进后的余弦相似度
* 对向量做了中心化处理（向量减去向量均值）
* 适用于测试两个向量的变化趋势是否一致
* 不适用于计算布尔值向量

#### 公式
```math
R(p,q) = \frac{ \sum_{i=1}^{n}((p_i - \bar{p}) (q_i - \bar{q})) }
                    { \sqrt{\sum_{i=1}^{n}(p_i - \bar{p})^2}
                      \times  \sqrt{\sum_{i=1}^{n}(q_i - \bar{q})^2} }
```

#### 值
* 范围：-1 -- 1
* -1 表示负相关，1表示正相关


### Jaccard相似度

#### 特点
* 适用于计算布尔向量
* 适用于隐式反馈数据

#### 定义
* 两个集合中，交集元素个数占并集元素个数的比例

#### 公式

**一般形式**
```math
J(p,q)= \frac{ \sum_{i=0}^{n} p_i q_i  }
             { \sum_{i=0}^{n} p_i \mid q_i } 
```

**集合形式**
```math
J(p,q)= \frac{ \left | P \cap Q  \right | }
             { \left | P \cup  Q  \right | } 
```

## 平滑处理

### 作用
* 解决零概率问题
* 计算测试样本的概率时，如果某个分量在训练集中未出现过，导致整个样本的概率为0

### 思想
* 提高低概率，降低高概率，尽量使概率分布趋于均匀

### 方案

#### 拉普拉斯平滑（加一平滑）
* 方法：计算概率时，分子加一，分母加K（类别数目）

##### 公式
```math
g(x) = \frac{ n_x + \alpha  }{ total + \alpha c } 
```
**说明**
* n为x类别的次数
* total为总次数
* c为类别数目
* $\alpha$为平滑系数，一般取1，值越大结果越接近均匀分布


## 推荐算法

### 基于内容

#### 原理
* 使用NLP技术来挖掘文本的特征信息
* 根据文本的特性来获得用户偏好，进行推荐

### 协同过滤

#### 基于用户

##### 原理
* 计算用户之间的相似度
* 找出相似用户喜欢的物品
* 对物品进行打分，推荐分数高的

#### 基于物品

##### 原理
* 计算物品之间的相似度
* 对物品进行打分，推荐分数高的

#### 基于模型 - 关联算法

##### 原理
* 通过物品数据挖掘频繁集
* 如果用户购买了频繁集中的部分物品，可以推荐其他物品

##### 算法
* Apriori
* FP Tree

#### 基于模型 - 聚类算法

##### 原理

###### 基于用户
1. 将用户聚类，分成不同的人群
2. 计算相同人群中评分最高的物品，推荐给用户

###### 基于物品
1. 将物品聚类，分成不同的类型
2. 将用户评分高的物品的相似同类物品，推荐给用户

##### 算法
* K-Means
* BIRCH

#### 基于模型 - 回归算法

##### 原理
* 通过回归模型来预测用户给某物品的评分

##### 算法
* Ridge回归
* 回归树
* 支持向量回归

#### 基于模型 - 矩阵分解
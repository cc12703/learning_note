[TOC]

# 模型与算法

## 概述

### 经典算法
* k最近邻（KNN）
* 决策树

### 概率图算法
* 隐马尔可夫模型（HMM）


### 回归算法
* 线性回归

#### 思路
* 用观测值拟合一条曲线、函数
* 用拟合出的曲线、函数去预测未知值


### 分类算法
* 感知机
* 逻辑回归
* 朴素贝叶斯


### 降维算法
* 主成分分析（PCA）
* 线性判别分析（LDA）

#### 作用
* 使数据更容易查看、分类
* 有利于数据压缩

#### 要点
* 降维时要最大限度地保留原来的方差

### 聚类算法
* k均值（K-means）
* DBSCAN

#### 特点
* 解决问题的效果不如有监督算法
* 无监督导致对结果的评估比较困难
* 不同的算法得到的结果差异比较大
* 同一算法不同参数得到的结果差异也会比较大


## k最近邻（KNN）

### 概述
* 一种监督式学习算法

### 优点
* 思想简单，也用于回归和分类
* 用于非线性分类
* 训练的时间复杂度比较低
* 对数据没有假设，对异常点不敏感

### 缺点
* 预测计算量大，尤其特征数非常多的时候
* 训练是需要大量内存
* 样本不平衡时，对稀有类型的预测准确率低
* 可解释性不强

### 适用情况
* 样本容量比较大的类域的自动分类

### 要点

#### k值选择

##### 方法
* 选择一个较小的值，通过交叉验证选择一个合适的值

##### 值小
* 训练误差小，泛化误差大
* 模型整体变复杂，容易发生过拟合

##### 值大
* 泛化误差小，训练误差大
* 模型整体变简单，容易预测错误

#### 距离度量
* 欧式距离（最常用）
* 曼哈顿距离
* 闵可夫斯基距离（Minkowski）

#### 决策规则
* 回归：平均法，输出最近的k个样本的平均值
* 分类：多数表决法，选最近的k个样本数量最多的那个类别

### 实现方案

#### KD树

##### 原理
* 通过建立KD树，来减少无效的最近邻搜索

##### 步骤
1. 建树
1. 搜索最近邻
1. 预测

#### 球树（BallTree）
* 进一步优化KD树的搜索效率


### 扩展
* 限定半径最近邻：解决样本中某类别的样本非常少的情况
* 质心算法：通常用于文本分类


## 决策树

### 概述

#### 优点
* 原理简单，可用于回归、也可用于分类
* 数据基本不需要预处理
* 预测计算量小
* 既可以处理离散值，也可以处理连续值
* 解释性强
* 对异常点不敏感

#### 缺点
* 非常容易过拟合
* 结果容易随样本改动而剧烈变动
* 容易陷入局部最优
* 若某特征的样本比例过大，生成的树容易偏向这些特征


### 树组成
* 包括：一个根节点、若干中间节点、若干叶节点

#### 根节点
* 包含所有样本

#### 中间节点
* 每个节点对应一个属性测试
* 节点包含的样本会根据属性测试的结果划分到子节点中

#### 叶节点
* 包含决策结果


### 构建-划分选择

#### 目标
* 节点所包含的样本尽可能属于同一类别

#### 衡量标准

##### 信息增益
* 定义：按属性分类后，信息熵值的下降程度
* 选择：选择值最大的属性作为最优划分属性
* 公式
    $$
Gain(D,a) = Ent(D) - \sum_{v=1}^{V} 
                     \frac{|D^v|}{|D|}
                     Ent(D^v) 
    $$
    * D: 全样本集
    * $D^v$ 包含特定属性值的样本集
    * a: 属性
    * V: 属性a的可能取值
    * Ent() 集合的信息熵
    * |D| 集合的样本数
* 缺点
    * 对可取值数目较多的属性有所偏好

##### 增益率
* 选择：选择值最大的作为最后划分属性
* 公式
    $$
Gain\_ratio(D,a) = \frac{ Gain(D,a) }
                 { - \sum_{v=1}^{V} 
                          \frac{|D^v|}{|D|} log_2(\frac{|D^v|}{|D|})
                  } 
    $$
* 缺点
    * 对可取值数目比较少的属性有所偏好

##### 基尼指数
* 定义：按属性分类后子集合基尼数的加权和
* 选择：选择值最小的属性作为最优划分属性
* 公式
    $$
Gini\_index(D,a) = \sum_{v=1}^{V} 
                          \frac{|D^v|}{|D|} Gini(D^v) 
    $$
    * Gini() 集合的基尼值


### 剪枝

#### 目的
* 防止决策树过拟合

#### 策略

##### 预剪枝
* 原理：在构造树的同时进行剪枝
* 过程
    * 每个节点在划分前先进行泛化能力评估
    * 若划分不能带来泛化能力的提升，则停止划分并将其标记为叶节点
* 泛化能力评估：预留一部分数据作为验证集
* 优点
    * 减少训练时间开销
* 缺点
    * 有欠拟合的风险

##### 后剪枝
* 原理：在构造一棵完整的树后，再进行剪枝
* 过程
    * 生成一棵完整的树
    * 从底向上对中间节点进行泛化能力评估
    * 若去除该节点会带来性能提升，则将其替换为叶节点
* 优点
    * 欠拟合风险比较小
* 缺点
    * 训练时间开销比较大


### 连续值
* 需要对连续值进行离散处理


### 实现方案

#### ID3

##### 特点
* 只支持分类任务
* 使用多叉树
* 使用 信息增益 选择特征
* 不支持连续值特征
* 不支持缺失值
* 不支持剪枝

##### 缺点
* 容易选择取值比较多的特征
* 容易过拟合

#### C4.5

* ID3的改进版本

##### 特点
* 只支持分类任务
* 使用多叉树
* 使用 信息增益比 选择特征
* 支持连续值
* 支持缺失值
* 支持剪枝

##### 改进点
* 将连续特征离散化
* 使用正则化系数进行剪枝操作

#### CART

##### 特点
* 支持分类、回归任务
* 使用二叉树
* 使用基尼系数、均方差 选择特征
* 支持连续值
* 支持缺失值
* 支持剪枝

##### 树

###### 回归树
* 样本输出是连续值
* 使用均 方差 选择特征
* 使用叶子节点的均值或中位数来预测结果

###### 分类树
* 样本输出是离散值
* 使用 基尼系数 选择特征
* 使用叶子节点中概率最大的类别作为预测类别


### 注意点

#### 若样本少，特征非常多
* 决策树容易过拟合
* 推荐先使用降维算法压缩特征

#### 多用可视化来观察数据的拟合情况
* 可以先限制树的深度

#### 若样本是稀疏的
* 拟合前转为csc格式
* 预测前转为csr格式



## 线性回归

### 含义
* 用一条直线去拟合一组观测数据

### 模型

#### 方程

**一般形式**
$$
h_\theta(x) = \sum_{i=0}^{n} \theta_ix_i
$$

**矩阵形式**
$$
h_\theta(X) = \frac{1}{ 1 + e^{-X\theta} } 
$$

#### 损失函数 

**一般形式**
```math
J(\theta) = \frac{1}{2}
            \sum_{i=1}^{m}
            ( h_{\theta}(X_i) - y_i)^2  
```

**矩阵形式**
```math
J(\theta) = \frac{1}{2} (X\theta - Y)^T(X\theta - Y)
```

**L1正则化-矩阵形式**
```math
J(\theta) = \frac{1}{2} (X\theta - Y)^T(X\theta - Y)
             + \alpha {\left \| \theta \right \|}_1 
```

**L2正则化-矩阵形式**
```math
J(\theta) = \frac{1}{2} (X\theta - Y)^T(X\theta - Y) 
            +   \frac{1}{2} \alpha {\left \| \theta \right \|}_2 
```


#### 求解
* 最小二乘法
* 梯度下降法

#### 评价方法
* R平方


### 正则化

#### 目的
* 防止模型过拟合，增加泛化能力

#### Lasso回归
* 定义：在损失函数上增加一个L1正则化的项
* 作用：使得一些特征的系数变小，甚至直接变成0

#### Ridge回归
* 定义：在损失函数上增加一个L2正则化项
* 作用：缩小回归系数，使得模型相对比较稳定

## 逻辑回归

### 二元回归（二分类学习）

#### 模型
* 对线性回归的结果，使用sigmoid函数进行转换
* 输出值为 0 -- 1 之间
* 判断分类结果时，可以使用0.5作为阀值

#### 方程

**矩阵形式**
```math
h_\theta(X) = \frac{1}{ 1 + e^{-X\theta} } 
```

#### 损失函数

**一般形式**
```math
J(\theta) = - \sum_{i=1}^{m}( 
                y_i log(h_\theta(x_i)) 
             +  (1 - y_i)log(1 - h_\theta(x_i))
            ) 
```


**矩阵形式**
```math
J(\theta) = - Y^Tlog(h_\theta(X)) - (E - Y)^Tlog(E - h_\theta(X))
```

* E为全1向量


### 多元回归（多分类学习）

#### 思路
* 拆解法，将任务拆为若干个二分类任务

#### 步骤
1. 将问题拆分为多个二分类任务
2. 对每个二分类任务，训练一个分类器
3. 预测时，对分类器结果进行集成，获取最终的多分类结果

#### 拆分策略

##### OvO（一对一）
* 将N个类别两两配对（一个正例，一个反例），生成N*(N - 1)/2个二分类任务

##### OvR（一对其余）
* 将一个类别作为正例，其余为反例，生成N个二分类任务

##### MvM（多对多）
* 每次将若干类别作为正例，若干其他类别作为反例
* 使用ECOC（纠错输出码）来构造正、反例
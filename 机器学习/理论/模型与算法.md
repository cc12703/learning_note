[TOC]

# 模型与算法

## 概述

### 经典算法
* k最近邻（KNN）
* 决策树

### 概率图算法
* 隐马尔可夫模型（HMM）

### 回归算法
* 线性回归

#### 思路
* 用观测值拟合一条曲线、函数
* 用拟合出的曲线、函数去预测未知值

### 分类算法
* 感知机
* 逻辑回归
* 朴素贝叶斯

### 降维算法
* 主成分分析（PCA）
* 线性判别分析（LDA）

#### 作用
* 使数据更容易查看、分类
* 有利于数据压缩

#### 要点
* 降维时要最大限度地保留原来的方差

### 聚类算法
* k均值（K-means）
* DBSCAN

#### 特点
* 解决问题的效果不如有监督算法
* 无监督导致对结果的评估比较困难
* 不同的算法得到的结果差异比较大
* 同一算法不同参数得到的结果差异也会比较大

### 集成算法
* 随机森林
* Adaboost

#### 特点
* 既可以用于分类问题、也可以用于回归问题
* 基础模型一般都是树模

#### 策略

##### bagging（装袋）
* 思路：将多个基础模型并行集成
* 步骤
    1. 对数据集进行随机采样
    2. 分别训练多个基础模型
    3. 将多个预测结果求平均值
* 例子：随机森林

##### boosting（提升）
* 思路：将基础模型一个一个串行加入
* 步骤
    1. 生成一个基础模型
    2. 不断加入新模型，前提加入后会使整体效果变好
    3. 将多个模型的结果加在一起
* 例子：Adaboost、随机梯度提升（GBM）

##### stacking
* 思路：集成不同算法的模型
* 步骤
    1. 选择m个不同分类器对数据进行建模
    2. 将第一阶段的结果当作数据特征传入总分类器
    3. 由总分类器获得最终结果
* 细节
    * 将数据分成多份，每个分类器使用其中的一份


## 分类

### 线性算法
* 线性回归
* 感知机
* 逻辑回归
* 支持向量机（SVM）
* 线性判别分析（LDA）


## k最近邻（KNN）

### 概述
* 一种监督式学习算法

### 优点
* 思想简单，也用于回归和分类
* 用于非线性分类
* 训练的时间复杂度比较低
* 对数据没有假设，对异常点不敏感

### 缺点
* 预测计算量大，尤其特征数非常多的时候
* 训练是需要大量内存
* 样本不平衡时，对稀有类型的预测准确率低
* 可解释性不强

### 适用情况
* 样本容量比较大的类域的自动分类

### 要点

#### k值选择

##### 方法
* 选择一个较小的值，通过交叉验证选择一个合适的值

##### 值小
* 训练误差小，泛化误差大
* 模型整体变复杂，容易发生过拟合

##### 值大
* 泛化误差小，训练误差大
* 模型整体变简单，容易预测错误

#### 距离度量
* 欧式距离（最常用）
* 曼哈顿距离
* 闵可夫斯基距离（Minkowski）

#### 决策规则
* 回归：平均法，输出最近的k个样本的平均值
* 分类：多数表决法，选最近的k个样本数量最多的那个类别

### 实现方案

#### KD树

##### 原理
* 通过建立KD树，来减少无效的最近邻搜索

##### 步骤
1. 建树
1. 搜索最近邻
1. 预测

#### 球树（BallTree）
* 进一步优化KD树的搜索效率


### 扩展
* 限定半径最近邻：解决样本中某类别的样本非常少的情况
* 质心算法：通常用于文本分类


## 决策树

### 概述

#### 优点
* 原理简单，可用于回归、也可用于分类
* 数据基本不需要预处理
* 预测计算量小
* 既可以处理离散值，也可以处理连续值
* 解释性强
* 对异常点不敏感

#### 缺点
* 非常容易过拟合
* 结果容易随样本改动而剧烈变动
* 容易陷入局部最优
* 若某特征的样本比例过大，生成的树容易偏向这些特征


### 树组成
* 包括：一个根节点、若干中间节点、若干叶节点

#### 根节点
* 包含所有样本

#### 中间节点
* 每个节点对应一个属性测试
* 节点包含的样本会根据属性测试的结果划分到子节点中

#### 叶节点
* 包含决策结果


### 构建-划分选择

#### 目标
* 节点所包含的样本尽可能属于同一类别

#### 衡量标准

##### 信息增益
* 定义：按属性分类后，信息熵值的下降程度
* 选择：选择值最大的属性作为最优划分属性
* 公式
    $$
Gain(D,a) = Ent(D) - \sum_{v=1}^{V} 
                     \frac{|D^v|}{|D|}
                     Ent(D^v) 
    $$
    * D: 全样本集
    * $D^v$ 包含特定属性值的样本集
    * a: 属性
    * V: 属性a的可能取值
    * Ent() 集合的信息熵
    * |D| 集合的样本数
* 缺点
    * 对可取值数目较多的属性有所偏好

##### 增益率
* 选择：选择值最大的作为最后划分属性
* 公式
    $$
Gain\_ratio(D,a) = \frac{ Gain(D,a) }
                 { - \sum_{v=1}^{V} 
                          \frac{|D^v|}{|D|} log_2(\frac{|D^v|}{|D|})
                  } 
    $$
* 缺点
    * 对可取值数目比较少的属性有所偏好

##### 基尼指数
* 定义：按属性分类后子集合基尼数的加权和
* 选择：选择值最小的属性作为最优划分属性
* 公式
    $$
Gini\_index(D,a) = \sum_{v=1}^{V} 
                          \frac{|D^v|}{|D|} Gini(D^v) 
    $$
    * Gini() 集合的基尼值


### 剪枝

#### 目的
* 防止决策树过拟合

#### 策略

##### 预剪枝
* 原理：在构造树的同时进行剪枝
* 过程
    * 每个节点在划分前先进行泛化能力评估
    * 若划分不能带来泛化能力的提升，则停止划分并将其标记为叶节点
* 泛化能力评估：预留一部分数据作为验证集
* 优点
    * 减少训练时间开销
* 缺点
    * 有欠拟合的风险

##### 后剪枝
* 原理：在构造一棵完整的树后，再进行剪枝
* 过程
    * 生成一棵完整的树
    * 从底向上对中间节点进行泛化能力评估
    * 若去除该节点会带来性能提升，则将其替换为叶节点
* 优点
    * 欠拟合风险比较小
* 缺点
    * 训练时间开销比较大


### 连续值
* 需要对连续值进行离散处理


### 实现方案

#### ID3

##### 特点
* 只支持分类任务
* 使用多叉树
* 使用 信息增益 选择特征
* 不支持连续值特征
* 不支持缺失值
* 不支持剪枝

##### 缺点
* 容易选择取值比较多的特征
* 容易过拟合

#### C4.5

* ID3的改进版本

##### 特点
* 只支持分类任务
* 使用多叉树
* 使用 信息增益比 选择特征
* 支持连续值
* 支持缺失值
* 支持剪枝

##### 改进点
* 将连续特征离散化
* 使用正则化系数进行剪枝操作

#### CART

##### 特点
* 支持分类、回归任务
* 使用二叉树
* 使用基尼系数、均方差 选择特征
* 支持连续值
* 支持缺失值
* 支持剪枝

##### 树

###### 回归树
* 样本输出是连续值
* 使用均 方差 选择特征
* 使用叶子节点的均值或中位数来预测结果

###### 分类树
* 样本输出是离散值
* 使用 基尼系数 选择特征
* 使用叶子节点中概率最大的类别作为预测类别


### 注意点

#### 若样本少，特征非常多
* 决策树容易过拟合
* 推荐先使用降维算法压缩特征

#### 多用可视化来观察数据的拟合情况
* 可以先限制树的深度

#### 若样本是稀疏的
* 拟合前转为csc格式
* 预测前转为csr格式



## 线性回归

### 概述
* 思想：用一条直线去拟合一组观测数据
* 目标函数：最小化均方误差
* 求解算法：梯度下降法

### 模型
#### 映射函数
$$
f(x) = {\omega} \cdot x + b
$$

说明
* $\omega$ 和 x 都是向量

#### 目标函数 
$$
\min_{\omega} \frac{1}{2} (X\omega - Y)^T (X\omega - Y)
$$


### 求解
#### 更新方程
$$
\omega = \omega - \alpha X^T (X\omega - Y)
$$

说明
* $\alpha$ 为步长


### 正则化
#### 目的
* 防止模型过拟合，增加泛化能力

#### Lasso回归
* 定义：在目标函数上增加一个L1正则化的项
* 作用：使得一些特征的系数变小，甚至直接变成0

##### 目标函数
```math
\min_{\omega} \frac{1}{2} (X\omega - Y)^T(X\omega - Y)
             + \alpha {\left \| \omega \right \|}_1 
```

#### Ridge回归
* 定义：在目标函数上增加一个L2正则化项
* 作用：缩小回归系数，使得模型相对比较稳定

##### 目标函数
```math
\min_{\omega} \frac{1}{2} (X\omega - Y)^T(X\omega - Y) 
            +  \alpha {\left \| \omega \right \|}_2 
```


## 感知机

### 概述
* 二分类模型，要求数据是线性可分的
* 思想：使用分离超平面来将数据分成两个部分
* 目标函数：最小化所有误分类点到分离超平面的总距离
* 求解算法：随机梯度下降法

### 模型
#### 分离超平面
$$
\omega x + b = 0
$$
说明
* $x \in R^n$, n为特征空间大小
* $\omega$ 和 $x$ 都是向量

#### 映射函数
$$
f(x) = sign(\omega x + b)
$$
说明
* sign为符号函数，输出+1、-1

### 原始形式
#### 目标函数
$$
\min_{\omega,b}L(\omega, b) = - \sum_{ x_i \in M } y_i (\omega * x_i + b)
$$
说明
* M为所有误分类点的集合

#### 推导过程
1. 点到分离平面的距离  
$$ \frac{|\omega x_0 + b|}{||\omega||} $$
1. 误分类点到分离平面的距离
$$ - y_i (\omega \cdot x_i + b) > 0 $$
1. 所有误分类点到分离平面的距离
$$
-\frac{1}{||\omega||} \sum_{ x_i \in M } y_i (\omega * x_i + b)
$$
1. 分子和分母都含有$\omega$，有固定的倍数关系，所以可以固定分母为1
    * 不会影响求极值
    * 不会影响终止条件
$$
-\sum_{ x_i \in M } y_i (\omega * x_i + b)
$$

#### 求解
##### 更新公式
$ \omega = \omega + \eta y_i x_i $
$ b = b + \eta y_i $
说明
* $\eta$ 为步长

##### 误分类判断公式
$$ y_i (\omega \cdot x_i + b) \le 0 $$

##### 步骤
1. 设置初始值：$\eta$, $\omega$, $b$
1. 从训练集中选择一个样本
1. 使用误分类判断公式，判断是否是误分类样本
1. 若是误分类样本，使用更新公式更新 $\omega$, $b$
1. 进入步骤2，直到训练集中没有误分类样本


### 对偶形式
#### 目的
* 优化算法的执行速度（在特征多，样本少时起作用）

#### 原理
* 将$\omega$, $b$ 表示为 $x$，$y$的线性组合
* 将每轮迭代复杂度，从**特征空间维度**转移到了**训练集大小**上

##### 公式
$$
\omega = \sum_{i=1}^{N} \alpha_i y_i x_i
$$
$$
b = \sum_{i=1}^{N} \alpha_i y_i
$$
说明
* $\alpha_i = n_i \eta$, n为样本被误分类的次数
* N为样本数据个数

#### 求解
##### 更新公式
$ \alpha_i = \alpha_i + \eta $
$ b = b + \eta y_i $
说明
* $\eta$ 为步

##### 误分类判断公式
$$ 
y_i (\sum_{j=1}^{N} (\alpha_j y_j x_j) \cdot x_i + b) \le 0 
$$
说明
* 可以预先计算出样本数据的內积，称为Gram矩阵
$$ G = [x_i \cdot x_j]_{N \times N} $$

 


## 逻辑回归
### 概述
* 二分类模型, 属于对数线性模型
* 目标函数：通过二项分布，使用极大似然法推导出
* 求解算法：梯度下降算法

### 模型
#### 函数
$$
f(x) = \frac{1}{ 1 + e^{ - ({\omega}^Tx + b)} }
$$

说明
* $\omega$ 和 x 都是向量
* 对线性回归的结果，使用sigmoid函数进行转换
* 输出值为 0 -- 1 之间
* 判断分类结果时，可以使用0.5作为阀值

#### 目标函数
$$
\min_{\omega} - Y^T log(h_{\omega}(X)) - (E - Y)^T log(E - h_{\omega}(X))
$$
说明
* E为全1向量
* $ h_{\omega}(x) = \frac{1}{1 + e^{-x \omega}}$

##### 推导过程
1. 根据二元分布，获的概率函数
$$
P(y|x, \omega) = h_{\omega}(x)^y (1 - h_{\omega}(x))^{1 - y}
$$
1. 获得似然函数
$$
L(\omega) = \prod_{i=1}^{m} (h_{\omega}(x_i))^{y_i} 
                (1 - h_{\omega}(x_i))^{1-y_i} 
$$
1. 对似然函数进行对数化，取反即可得到目标函数


### 求解（梯度下降法）
#### 更新方程
$$
\omega = \omega - \alpha X^T (h_{\omega}(X) - Y)
$$
说明
* $\alpha$ 为步长
* $X, Y$都是向量




## 支持向量机（SVM）

### 概述
* 二分类模型
* 使用分离超平面来分隔样本数据
* 目标函数：最大化样本到超平面的间隔
* 求解算法：凸二次规划优化法

#### 支持向量
* 样本中与分离超平面距离最近的样本实例
* 在分离超平面中起着决定性作用

#### 分类
* 线性可分SVM：使用硬间隔最大化，样本需要线性可分
* 线性SVM：使用软间隔最大化，样本要近似线性可分
* 非线性SVM：使用核技巧、软间隔最大化，样本线性不可分

### 间隔
* 一个样本点距离分离超平面的远近
* 该间隔可以近似表示分类预测的置信度

#### 函数间隔
$$
\hat{\gamma_{i}} = y_i (w \cdot x_i + b) 
$$

#### 几何间隔
$$
\gamma_{i} = y_i (\frac{\omega}{\left\| \omega \right\|} \cdot x_i 
    + \frac{b}{\left\| \omega \right\|} ) 
$$

### 线性可分SVM算法
#### 原始优化问题
$$
\min_{\omega, b} \frac{1}{2} {\left \| \omega  \right \|}^2
$$
s.t. 
$$ 
y_i(\omega \cdot x_i + b) - 1 \ge 0 , i = 1,2,3...,N 
$$

##### 求解步骤
1. 根据优化问题，求解出最优解 $ \omega^* , b^* $
1. 生成分类决策函数 $ f(x) = sign( \omega^* \cdot x + b^*) $

#### 对偶优化问题
$$
\min_{\alpha} \frac{1}{2} 
          \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j (x_i \cdot y_j)
           - \sum_{i=1}^{N} \alpha_i
$$
s.t.
$$ 
\sum_{i=1}^{N} \alpha_i y_i = 0
$$
$$
\alpha_i \ge 0, i=1,2,...,N
$$

##### 求解步骤
1. 根据优化问题，求解出最优解：$ \alpha^* = (\alpha_{1}^* + \alpha_{12}^*+ ... + \alpha_{N}^*)^T $
1. 计算出 $ \omega^* , b^* $
1. 生成分类决策函数 $ f(x) = sign( \omega^* \cdot x + b^*) $


### 线性SVM算法
#### 原始优化问题
$$
\min_{\omega, b, \xi} \frac{1}{2} {\left \| \omega  \right \|}^2
                       + C \sum_{i=1}^{N}\xi_i
$$
s.t.
$$ 
y_i(\omega \cdot x_i + b) - 1 \ge 1 - \xi_i , i = 1,2,3,...,N
$$
$$
\xi_i \ge 0 , i = 1,2,3,...,N
$$

说明
* C为调和系数， $\xi_i$为松弛变量

#### 对偶优化问题
$$
\min_{\alpha} \frac{1}{2} 
          \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j (x_i \cdot y_j)
           - \sum_{i=1}^{N} \alpha_i
$$
s.t.
$$ 
\sum_{i=1}^{N} \alpha_i y_i = 0
$$
$$
0 \le \alpha_i \le C, i=1,2,...,N
$$


### 非线性SVM算法
#### 优化问题
$$
\min_{\alpha} \frac{1}{2} 
          \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j K(x_i \cdot y_j)
           - \sum_{i=1}^{N} \alpha_i
$$
s.t.
$$ 
\sum_{i=1}^{N} \alpha_i y_i = 0
$$
$$
0 \le \alpha_i \le C, i=1,2,...,N
$$

说明
* K 

##### 求解步骤
1. 根据优化问题，求解出最优解：$ \alpha^* = (\alpha_{1}^* + \alpha_{12}^*+ ... + \alpha_{N}^*)^T $
1. 计算出 $ b^* $
1. 生成分类决策函数 $ f(x) = sign( \alpha_{i}^* y_i K(x, x_i) + b^*) $


### 核技巧
#### 思路
* 进行一个非线性变换，将非线性问题变换为线性问题
* 从原始空间映射到更高维的特征空间，使样本在特征空间线性可分

#### 核函数
##### 优点
* 在学习、预测时，只使用核函数
* 不显式定义映射函数

##### 定义
$$  K(x, z) = \Phi(x) \cdot \Phi(z) $$

说明
* $\Phi$为映射函数
* x为原始空间的样本
* z为特征空间的样本

##### 常用核
* 多项式核
* 高斯核
* 字符串核

#### SVM应用
* 将对偶问题中目标函数的內积用核函数代替


### SMO算法
* 全称：序列最小最优化算法 


## 朴素贝叶斯

### 要点
* 使用贝叶斯公式
* 假设样本特征之间是独立的

### 模型

#### 贝叶斯公式
$$
P(Y_k|X)= \frac{ P(X|Y_k)P(Y_k) }
               { \sum_k P(X| Y=Y_k)P(Y_k) } 
$$

#### 模型公式
$$
C_{result} =  \underbrace{argmax}_{C_k}
              P(Y = C_k)
              \prod_{j=1}^{n} P( X_j = X_{j}^{(test)} | Y = C_k)
$$

**说明**
* n 样本特征个数
* C 类别值，一共k个

### 步骤
1. 计算Y的先验概率
2. 计算第k个类别下的第j个特征的条件概率
3. 通过模型公式，计算所有的k个条件概率
4. 选择概率最大的作为预测类型

### 优点
* 小规模数据的表现比较好
* 能处理多分类
* 适合增量训练
* 对缺失数据不敏感

### 缺点
* 属性数较多、属性间相关性较大时，分类效果差
* 需要知道先验概率
* 对输入数据的表达形式很敏感

### 改进
* 如果有概率为零的情况，需要进行拉普拉斯平滑处理



## 主成分分析（PCA）

### 概述
* 一种非监督学习算法
* 假设数据符合高斯分布
* 选择样本投影具有最大方差的方向（分类信息依赖于方差）

### 思想
* 找出数据里最主要的方面，用最主要方面来代替原始数据

### 降维标准
* 基于最小投影距离：样本点到超平面的距离足够近
* 基于最大投影方差：样本点在超平面上的投影尽可能的分开

### 主成分
* 定义：样本集的**协方差矩阵**的前n个**特征值**对应的**特征向量矩阵**

### 步骤
1. 对样本进行中心化
2. 计算样本的协方差矩阵
3. 对矩阵进行特征值分解
4. 取出最大的 n 个特征值对应的特征向量
5. 将所有n个特征向量标准化，生成特征向量矩阵
6. 使用该矩阵对每个样本进行转化，生成新样本

### 优点
* 只需要使用方差信息，不受数据集以外的因素影响
* 各主成分之间正交，可以消除原数据之间相互影响的因素
* 计算简单，主要使用特征值分解



## 线性判别分析（LDA）

### 概述
* 一种监督学习算法
* 假设数据符合高斯分布
* 选择分类性能最好的投影方向（分类信息依赖于均值）

### 思想
* 使投影后的类内方差最小，类间方差最大
* 将数据在低维度上进行投影
* 希望同一种类别数据的投影点尽可能的接近
* 希望不同类别数据的类别中心尽可能的分离

### 步骤
1. 计算类内散度矩阵 Sw
2. 计算类间散度矩阵 Sb
3. 计算出投影矩阵 W
4. 对样本中的每个样本特征，转化为新的样本

### 缺点
* 不适合对非高斯分布样本进行降维
* 最多降维到 k - 1
* 在样本分类信息依赖方差而不是均值时，效果不好


## k均值（K-means）

### 特点
* 基于距离计算来聚类
* 基于数据本身的特性，将一组数据划分为多个簇
* 根据数据离哪个中心点最近来决定属于哪个簇

### 步骤

#### 划分
1. 随机选择k个中心点
2. 计算每个数据点到中心点的距离，分配给离其最近的中心点
3. 基于簇重新计算中心点，将中心点移动到簇中所有点的中心
4. 重复步骤2，3，直到中心点不再移动

#### 预测
1. 计算新数据点到每个簇中心点的距离
1. 分配给离数据点最近的簇

### 优点
* 快速、简单
* 效果通常还是不错的
* 可解释性比较强

### 缺点
* k值比较难确定
* 在球形簇上效果比较好，其他类型效果一般

### 注意点
* k的选择：从一个比较小的值开始，逐渐增加直到方差不会有大的改善
* 距离度量：使用欧式距离，余弦相似度
* 局部最小值：进行多次聚类，最后将结果平均
* 标记簇：需要研究数据，找出每个簇的意义
* 数据标准化
    * 一定要对数据进行缩放，以达到标准化
    * 在标准化的数据上效果最好



## DBSCAN

### 特点
* 一种基于密度的聚类算法
* 常用于异常检测

### 概念
* r-邻域：给定对象半径r内的邻域
* 核心点：r-邻域内的数据点
* 边界点：落在核心点邻域内的数据点
* 离群点：即不是核心点，也不是边界点

### 步骤
1. 随机选择一个数据点
2. 使用r半径画园
3. 凡是能被其及其下属圈到的数据点都属于同一类别
4. 若该类型已发展完成，就重复1-3步骤，直到所有数据点都处理过

### 优点
* 可以对任意形状的稠密度数据集进行聚类
* 适合于检测任务，寻找离群点
* 不需要指定类别数目

### 缺点
* 如果样本密度不均匀，效果比较差
* 半径选择比较难，导致的结果差异比较大    



## 隐马尔可夫模型（HMM）

### 概述
* 一种有向图模型
* 一种结构最简单的动态贝叶斯网

### 组成

#### 状态变量（隐变量）
* 是隐藏的，不可被观测的
* 表示第 i 时刻的系统状态，使用 Y 表示

#### 观测变量
* 表示第 i 时刻的观测值，使用 X 表示

#### 依赖关系
* 任一时刻，观测变量的值仅依赖于状态变量
* 第 t 时刻的状态仅依赖于第 t - 1 时刻的状态

#### 参数

##### 状态转移概率
* 定义：模型在各个状态间转移的概率

##### 输出观测概率
* 定义：模型根据当前状态获得各个观测值的概率

##### 初始状态概率
* 定义：模型在初始时各个状态出现的概率

### 观测序列生成
1. 设置t = 1, 根据初始状态概率生成隐藏变量 Yt
2. 根据隐藏变量 Yt 和 输出观测概率，生成观测变量 Xt
3. 根据隐藏变量 Yt 和 状态转移概率，生成隐藏变量 Yt+1
4. 若 t < n， 则 t += 1，进入步骤2



## 随机森林

### 思想
* 生成多个不同的树模型，将结果平均

### 概念
* 森林：多个决策树
* 随机（二重随机性）
    * 随机数据采样，每颗树的训练数据来自数据集的一部分
    * 随机特征选择，每颗树选择所有特征的一部分

### 结果集成
* 分类任务求众数
* 回归任务求平均值

### 特点
* 并行形式：多个树模型之间是无关的，使用相同的参数
* 可以进行可视化展示
* 总是先使用最好的特征



## Adaboost

### 思路
* 给样本赋予不同的权重
* 做对的样本，给较低权重
* 做错的样本，给较高权重，让模型更重视





## 相似度计算

### 欧式距离

#### 特点
* 使用距离来表示相似度
* 结果是一个绝对的客观值
* 适用于分析用户能力模型之间的差异：消费能力、贡献内容

#### 定义
* 在欧式空间下的度量的距离

#### 公式
```math
E = \sqrt{ \sum_{i=1}^{n} ( x_{i} - y_{i} )^2 } 
```

#### 值
* 范围：0 -- 正无穷
* 通常需要转化为 [-1, 1] 或者 [0, 1]



### 余弦相似度

#### 特点
* 使用向量夹角来表示相似度
* 值与向量的长度无关，经过了向量长度归一化处理
* 适用于测量文本相似度、用户相似度、物品相似度

#### 定义
* 取向量夹角的余弦值

#### 公式

**一般形式**
```math
\cos(\Theta) = \frac{ \sum_{i=1}^{n}(x_i \times y_i) }
                    { \sqrt{\sum_{i=1}^{n}(x_i)^2}
                      \times  \sqrt{\sum_{i=1}^{n}(y_i)^2} } 
```

**矩阵形式**
```math
\cos(\Theta) = \frac{ a \bullet b }
                    { \left \| a \right \| 
                       \times \left \| b \right \| } 
```

#### 值
* 范围：-1 -- 1
* 1 表示夹角为0度，0表示夹角为90度，-1表示夹角为180度


### 皮尔逊相关系数

#### 特点
* 一种改进后的余弦相似度
* 对向量做了中心化处理（向量减去向量均值）
* 适用于测试两个向量的变化趋势是否一致
* 不适用于计算布尔值向量

#### 公式
```math
R(p,q) = \frac{ \sum_{i=1}^{n}((p_i - \bar{p}) (q_i - \bar{q})) }
                    { \sqrt{\sum_{i=1}^{n}(p_i - \bar{p})^2}
                      \times  \sqrt{\sum_{i=1}^{n}(q_i - \bar{q})^2} }
```

#### 值
* 范围：-1 -- 1
* -1 表示负相关，1表示正相关


### Jaccard相似度

#### 特点
* 适用于计算布尔向量
* 适用于隐式反馈数据

#### 定义
* 两个集合中，交集元素个数占并集元素个数的比例

#### 公式

**一般形式**
```math
J(p,q)= \frac{ \sum_{i=0}^{n} p_i q_i  }
             { \sum_{i=0}^{n} p_i \mid q_i } 
```

**集合形式**
```math
J(p,q)= \frac{ \left | P \cap Q  \right | }
             { \left | P \cup  Q  \right | } 
```

## 平滑处理

### 作用
* 解决零概率问题
* 计算测试样本的概率时，如果某个分量在训练集中未出现过，导致整个样本的概率为0

### 思想
* 提高低概率，降低高概率，尽量使概率分布趋于均匀

### 方案

#### 拉普拉斯平滑（加一平滑）
* 方法：计算概率时，分子加一，分母加K（类别数目）

##### 公式
```math
g(x) = \frac{ n_x + \alpha  }{ total + \alpha c } 
```
**说明**
* n为x类别的次数
* total为总次数
* c为类别数目
* $\alpha$为平滑系数，一般取1，值越大结果越接近均匀分布


## 推荐算法

### 基于内容

#### 原理
* 使用NLP技术来挖掘文本的特征信息
* 根据文本的特性来获得用户偏好，进行推荐

### 协同过滤

#### 基于用户

##### 原理
* 计算用户之间的相似度
* 找出相似用户喜欢的物品
* 对物品进行打分，推荐分数高的

#### 基于物品

##### 原理
* 计算物品之间的相似度
* 对物品进行打分，推荐分数高的

#### 基于模型 - 关联算法

##### 原理
* 通过物品数据挖掘频繁集
* 如果用户购买了频繁集中的部分物品，可以推荐其他物品

##### 算法
* Apriori
* FP Tree

#### 基于模型 - 聚类算法

##### 原理

###### 基于用户
1. 将用户聚类，分成不同的人群
2. 计算相同人群中评分最高的物品，推荐给用户

###### 基于物品
1. 将物品聚类，分成不同的类型
2. 将用户评分高的物品的相似同类物品，推荐给用户

##### 算法
* K-Means
* BIRCH

#### 基于模型 - 回归算法

##### 原理
* 通过回归模型来预测用户给某物品的评分

##### 算法
* Ridge回归
* 回归树
* 支持向量回归

#### 基于模型 - 矩阵分解
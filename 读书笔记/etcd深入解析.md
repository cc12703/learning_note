
[TOC]

# etcd深入解析

## 一致性协议

### 一致性定义
* 不同的副本服务器认可同一份数据，一旦这些服务器对某份数据达成了一致。那么决定便是最终的决定。
* 一致性与结果的正确性无关，是系统对外呈现的状态是否一致

### 算法衡量标准
* 可终止性：非失败进程在有限时间内能够做出决定
* 一致性：所有进程必须对最终的决定达成一致
* 合法性：算法做出的决定值必须在其他进程（客户端）的期望值范围内

### 一致性模型
* 客户端：并发访问时如何获取更新过的数据的问题
* 服务端：更新如何复制分布到整个系统，以保证数据的最终一致性

#### 以数据为中心

##### 严格一致性（可线性化）
* 任何一次读都可以读到某个数据最近一次写的数据
* 系统中所有进程，看到的操作顺序，都与全局时钟下的顺序一致
* 传统意义上，单处理机遵守该一致性

##### 顺序一致性（可序列化）
* 所有的进程都以相同的顺序看到所有的修改
* 只需要所有进程都达成自己认为的一致就可以，不需要全局顺序保持一致
* 放弃了全局时钟的约束

##### 因果一致性
* 本地顺序：本进程中，事件执行的顺序即为本地因果顺序
* 异地顺序：如果读操作返回的是写操作的值，则写操作在顺序上一定在读操作前

##### 可串行化一致性
* 很弱：没有按时间或顺序排列界限
* 很强：需要一个线性顺序
* 允许对操作重新进行排序，只要顺序看起来是原子的即可


#### 以用户为中心

##### 最终一致性
* 如果更新的间隔时间比较长，那么所有的副本都能够最终达到一致性



### 复制状态机

#### 基本思想
* 系统由多个复制单元组成
* 每个单元均是一个状态机
* 其状态保存在一组状态变量中
* 状态机的状态只能通过外部命令改变

#### 状态机实现
* 基于操作日志
* 每个状态机存储一个包含一系列指令的日志
* 严格按照顺序逐条执行日志上的指令

#### 运行过程
1. 一致性模块接收外部命令，追加到操作日志中。并与其他服务器上的一致性模块通信
1. 保证每个服务器上的操作日志都以相同的顺序包含相同的指令
1. 一旦指令被正确复制，每个服务器的状态机都将按照日志的顺序来处理，并将结果返回给外部



## Raft协议

### 特点
* 把问题分解为 领袖选举、日志复制、安全性、成员关系变化
* 强领导人：日志只从领导人发向其他服务器
* 使用随机定时器来选举领导人，使得冲突解决更加简单和快速
* 使用**联合一致性**来解决成员关系变化问题

### 基本概念
* 使用了非对称节点关系模型
* 三类角色：Leader、Candidate、Follower
* 将时间划分为任意不同长度的 Term


#### 非对称关系
* 只有主节点拥有决策权
* 任意时刻有且仅有一个主节点
* 客户端只与主节点交互


#### Leader(领导人)
* 需要半数以上群众的选票
* 领导人产生后，又其昭告其他人，结束选举

#### Candidate(候选人)
* 竞选开始后，任何群众都可以成为候选人
* 竞选结束后，其他候选人都变回群众，接收领导

#### Follower(群众）
* 有领导人时，接收领导
* 无领导人时，进行竞选

#### Term(任期)
* 每个任期长度不同
* 任期号总是单调递增的
* 开始于一次领导人选举
* 若本次任期没有选出领导人，则自动进入下一个任期
* 起着逻辑时钟的作用，也可以检测过期信息


### 选举

#### 原则
* 一个任期内，一个节点最多只能为一个Candidate投票
* 按照先到先得的原则，投票给最早拉票的Candidate
* 使用 随机重试 来错开发起竞选的时间窗口

#### 触发
* Leader在任期内必须定期向其他节点广播心跳包
* 每个Follower都会启动一个选举定时器，时长都不相等
* Follower每次收到心跳包后，都将自己的选举定时器重置
* 若选举定时器超时了，Follower就会发起一次选举

#### 流程
1. 将本地的当前任期号加一
2. 切换到Candidate状态，并为自己投票
3. 向其他节点发送RequestVote RPC
4. 可能结果
    1. 若获取到大多数选票，则成为Leader
    1. 若发现其他节点赢得选举，则切换回Follower状态
    1. 若一段时间后，没有人赢得选举，则重新发起一次选举


### 日志复制

#### 结构
* 由有序编号的日志条目组成
* 日志条目包括：索引值，任期号，指令


#### 流程
1. 客户端向Leader发送写请求
2. Leader将写请求解析成指令，并追加到本地的日志文件中
3. Leader向每个Follower发送AppendEntries RPC
4. Follower通过一致性检查，选择位置追加Leader的日志条目
5. 一旦日志条目提交成功，Leader就将其应用到本地状态机，并向客户端返回结果
6. Leader发送AppendEntries RPC将成功提交的日志条目告知Follower
7. Follower收到通知后，将其应用到本地状态机

#### 保证一致性
* Leader在一个任期里在给定的一个日志索引位上，最多创建一条日志条目，且位置永远不会改变
* Follower在AppendEntries RPC中会进行简单的一致性检查（上一条日志条目的信息是否一致）
* Follower上与Leader冲突的日志会被Leader的日志强制覆写


### 安全性
* 保证任何Leader都拥有之前任期提交的全部日志条目
* 使用投票方式来阻止那些没有包含所有已提交日志条目的节点赢得选举
* 日志条目只有一个流向：从Leader流向Follower
* Leader永远不会覆盖已经存在的日志条目
* Leader在当前任期至少有一条日志被提交（即被超过半数的节点写盘

### 时序要求

* **等式：broadcastTime << electionTimeout << MTBF**
* broadcastTime ：节点之间进行PRC通信的平均时间（发送请求到接收响应）
* electionTimeout ：选举超时时间
* MTBF ：单个节点发生故障的平均时间间隔

#### 要求
* broadcastTime 应该比 electionTimeout 小一个数量级
* electionTimeout 应该比 MTBF 小几个数量级


### 压缩与快照

#### 快照文件
* 每个节点独立创建，只包含已经被提交的日志条目
* 存储了节点某一时间状态机的状态
* 全量式保存
* 在快照中存储少量元数据

#### 接收者流程
1. 如果节点的当前任期号大于Leader的任期号，则拒绝该快照
2. 如果第一个分块，则新建一个快照
3. 在指定偏移量处，将分块数据写入快照文件，并响应Leader
4. 若快照文件未传输完成，则继续等待更多的数据块
5. 若快照文件传输完成，则保存该文件，丢弃本地较旧的快照
6. 节点根据快照信息，找到要保留的日志项，将前面的日志项全部删除
7. 应用快照内容重置状态机，并加载其中的集群配置信息



### 性能

#### 指标
* Leader选举过程的收敛速度
* Leader宕机后，系统不可用的时间是多久

#### 随机化
* 选举超时时间只需要较小的随机化，就能够明显地降低因为选票被瓜分而导致的重新选举，从而降低选举耗时



## etcd概述

### 定义
* Go语言编写的
* 分布式的
* 高可用的
* 一致性的
* 键值存储系统

### 使用场景
* 服务发现
* 分布式锁
* 分布式数据队列
* 分布式通知和协调
* 主备选举


### VS ZooKeeper
* 更加稳定可靠
* 使用节点租约来实现服务发现
* 支持稳定的watch
* 支持MVCC(多版本并发控制）
* 支持更大的数据规模
* 性能更好


### 特点
* 基于Raft协议，通过复制日志文件的方式来保证数据的强一致性
* 默认数据一更新就落盘持久化
* 数据持久化存储使用WAL(write ahead log)格式



## etcd架构

### 设计特点
* 支持RESTful风格的API（HTTP + JSON）
* 使用Go语言编写，跨平台、部署和维护简单
* 使用Raft算法保证强一致性，可理解性好
* 支持TLS客户端安全认证
* 单实例支持每秒一千次以上的写操作

### 模块组成
* 网络层
* Raft模块
* 复制状态机：抽象模块，数据维护在内存中
* 存储模块：KV存储、WAL文件、快照管理

### 数据交互

#### 数据种类
* Leader向Follower发送心跳包，Follower向Leader回复消息
* Leader向Follower发送日志追加信息
* Leader向Follower发送快照数据
* Candidate向其他节点发送投票请求
* Follower将收到的写操作转发给Leader

#### 数据通道
* 使用protocol buffer格式进行编码
* 使用HTTP协议传输
* 分为：Stream类型和Pipeline类型

##### Stream类型
* 用于处理数据量较少的消息
* 节点之间只维护一个HTTP长连接，交替向链接中写入、读取数据


##### Pipeline类型
* 用于处理数据量大的消息
* 只通过短链接传输数据
* 可以并行发出多个消息

### 模块交互

#### 网络层和Raft模块
* 使用RaftNode来处理各种消息
* 通过Go的Channel来完成数据通信

#### server和客户端
* 所有客户端的请求会被处理，并传给Raft模块处理
* 通过HTTP协议进行通信

#### server之间
* 通过peer端口，使用HTTP进行通信




## etcd应用场景

### 服务注册和发现

#### 功能
* 了解集群中是否有进程在监听端口
* 通过名字就可以进行查找和链接

#### 条件
* 一个强一致性、高可用的服务存储目录
* 一种注册服务和检测服务健康状况的机制
* 一种查找和连接服务的机制

#### 方法
1. 在etcd中注册某个服务名字的目录
2. 在该目录下存储可用服务节点的IP
3. 使用时，从服务目录下查找可用的服务节点即可


### 消息发布和订阅

#### 方法
* 设置一个配置共享中心
* 消息提供者在中心发布消息
* 消息使用者则订阅他们关心的主题
* 一旦所关心的主题有消息发布，就会实时通知订阅者


#### 管理应用配置信息更新
1. 在应用启动时，主动从etcd获取一次配置信息
2. 在etcd上注册一个Watcher并等待

#### 日志收集
1. 在etcd上创建一个以应用为名的目录
2. 将相关机器的IP以子目录的形式存储
3. 设置一个递归Watcher


### 负载均衡

**使用etcd本身的分布式架构来实现**

#### 方法
1. etcd的每个核心节点都可以处理用户请求
2. 把数据量小但是访问频繁的数据直接存储到etcd中

#### 二级代码表
1. 在表中存储代码
2. 在etcd中存储代码所代表的具体含义
3. 业务系统在调用查表的过程中，可以从etcd中获取具体的含义

#### 负载均衡节点表
1. 由etcd来监控多个节点的状态
2. 请求过来时，可以轮询式地把请求转发给存活节点


### 通知和协调

#### 概述 
通过对etcd上同一目录进行注册，并使用Wacther来监控该目录

#### 低耦合的liveness probe
* 检测系统和被检测系统通过etcd上的目录进行管理而不是直接关联

#### 系统调度
1. 系统由 控制台 和 推送系统 组成
2. 控制台直接修改etcd上的指定目录
3. etcd会自动把变化通知给推送系统客户端
4. 推送系统在执行对应的任务

#### 工作汇报
1. 子任务启动后，在etcd中注册一个临时目录
2. 定时汇报自己的进度
3. 任务管理器就可以实时的获取到任务的进度


### 分布式锁

#### 保持独占
* 最终只有一个用户可以获取到锁
* 使用etcd提供的一套分布式锁原子操作API来实现

#### 控制时序
* 获取锁的所有用户都会进入等待队列，获取锁的顺序是全局唯一的
* 使用etcd提供的一个字段创建有序键API来实现


### 集群监控
* 使用Watcher机制，当某个节点消失时，Watcher会发现并告知用户
* 节点可以设置TTL key，主动发送心跳信号


### Leader竞选

#### 作用
对于一些需要长时间使用CPU或IO的操作，可以竞选出一个Leader来处理

#### 流程
1. 通过etcd的CAS机制竞选出Leader
2. Leader进行计算
3. Leader将计算结果分发到其他节点




## etcd实战


### 灾难恢复
* etcd v3可以通过快照和恢复机制来重建一个新的etcd集群

#### 做快照
* 通过命令 etcdctl snapshot save 获取
* 集群可以通过复制目录member/snap/db 来获取


### etcd网关
* 一个简单的TCP代理，用于向集群转发网络数据
* 对用户是透明且无状态
* 支持多个后端etcd服务器
* 只支持轮询的负载均衡策略
* 推荐方法：在每个节点上运行一个etcd网关，应用通过网关来访问集群


#### 不使用网关场景
* 在高性能场景下不推荐使用
* 在已经有服务发现机制时不推荐使用


### gRPC代理
* 一个运行在gRPC层、无状态的反向代理
* 用于降低核心etcd集群的请求负载
* 会合并watch以及为API请求绑定一个过期租约
* 可以保护集群免受大流量的冲击，缓存range request的结果
* 支持多个后端etcd服务器


### 硬件

#### CPU
* etcd需要大量的CPU资源
* 典型场景需要2、4个核
* 高负载场景下需要8，16个核

#### 内存
* etcd的内存占用相对较小
* 其性能取决于是否有足够的内存
* 内存主要用于缓解kv数据，跟踪watcher

* 典型场景需要8GB
* 如果Key和Watcher数量比较多，需要16-64G

#### 磁盘
* IO性能是影响etcd性能和稳定性最关键的因素

* etcd的一致性协议依赖于持久化元数据到日志文件
* 节点需要将每一个请求都写到磁盘中
* etcd对写磁盘的时延非常敏感
* etcd只需要适当的磁盘带宽就能正常运行

* 典型场景，需要高于50串行IOPS的磁盘
* 高负载场景，需要高于500串行IOPS的磁盘

#### 网络
* 低延时可以确保节点之间的快速通信
* 高带宽可以减少一个失败节点恢复正常的时间